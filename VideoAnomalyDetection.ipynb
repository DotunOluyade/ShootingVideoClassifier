{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DotunOluyade/ShootingVideoClassifier/blob/main/VideoAnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqG2VmTbSxG7"
      },
      "source": [
        "\n",
        "# Real-Time Crime Prevention using Video Anomaly Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014sqOa1E1R_"
      },
      "source": [
        "**Comparative** Analysis of Pretrained Video Classification models in detecting real-time shooting crime for peace and safety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAZ2GqfATJYM"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "A sub-set of UCF-Crime dataset is used for this research work. It contains 128 hours of video, comprising 1900 long untrimmed real world surveillance videos, with 13 realistic anomalies as well as normal activities (Sultani et al., *2018*).\n",
        "\n",
        "This research uses the shooting dataset only to fine-tune selected video classification pretrained models, compares and evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uparbOrXN11d"
      },
      "source": [
        "### Specify Dataset Location & Import Dependencies\n",
        "\n",
        "Import package dependencies and define variables for datasets and pretrained model.\n",
        "\n",
        "Video Swim a pure transformer based video modeling algorithm with its pretrained model is used for feature extraction and fine-tuned with the shooting dataset for classifying videos as shooting or non shooting videos (Liu et al.,2022).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "seFZ6F6PTF71"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "import cv2\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle as sk_shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "import glob\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "import pandas as pd\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "import gc\n",
        "import random\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Seeding to achieving consistent results across training and inference sessions.\n",
        "\"\"\"\n",
        "\n",
        "# Seed value\n",
        "seed_value = 42\n",
        "\n",
        "# Set python built-in pseudo-random generator\n",
        "random.seed(seed_value)\n",
        "\n",
        "# Set numpy pseudo-random generator\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# Set tensorflow pseudo-random generator\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "# Set PYTHONHASHSEED environment variable\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "\n",
        "base_directory = '/content/drive/MyDrive/VideoAnomalyDetection/'\n",
        "base_dataset_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/'\n",
        "saved_model_path = '/content/drive/MyDrive/VideoAnomalyDetection/pretrained/Video Swin Transformer/TFVideoSwinB_K600_IN22K_P244_W877_32x224'\n",
        "train_file_path = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Anomaly_Train_2.txt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Dataset"
      ],
      "metadata": {
        "id": "DvJzSHoNBfA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_video_paths_from_file(file_path, base_dataset_dir):\n",
        "    full_file_path = os.path.join(base_dataset_dir, file_path)\n",
        "\n",
        "    with open(full_file_path, 'r') as file:\n",
        "        # Concatenate base_dataset_dir with each path read from the file\n",
        "        paths = [os.path.join(base_dataset_dir, line.strip()) for line in file]\n",
        "\n",
        "    return paths\n",
        "\n",
        "def filter_video_paths(paths, filter_keyword):\n",
        "    return [path for path in paths if filter_keyword.lower() in path.lower()]\n",
        "\n",
        "\n",
        "def filter_video_paths_by_anomaly_type(paths, anomaly_type):\n",
        "    \"\"\"Filter video paths based on a specific anomaly type.\"\"\"\n",
        "    return [path for path in paths if anomaly_type.lower() in path.lower()]\n",
        "\n",
        "\n",
        "# Read video paths from the file\n",
        "video_paths = read_video_paths_from_file(train_file_path,base_dataset_dir)\n",
        "\n",
        "# Use \"Shooting\" and \"Normal\" to distinguish the types\n",
        "anomaly_videos = filter_video_paths(video_paths, 'Shooting')\n",
        "normal_videos = filter_video_paths(video_paths, 'Normal')\n",
        "\"\"\"\n",
        "for anomaly_path in anomaly_videos:\n",
        "    print(anomaly_path)\n",
        "\n",
        "for normal_path in normal_videos:\n",
        "    print(normal_path)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KHMuiOcnQZP0",
        "outputId": "6c581c32-c0d2-42ea-9895-539bd0d0ddbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor anomaly_path in anomaly_videos:\\n    print(anomaly_path)\\n\\nfor normal_path in normal_videos:\\n    print(normal_path)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pm7z6-3UOec"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Resize, normalize and split videos into frames. Batch frames in 32 segments.\n",
        "\n",
        "Use data generators to prevent loading large datset into memory at once, to better utilize memory for resource constrained environement.\n",
        "\n",
        "Generator loads and preprocess 32 frames at each interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "77GImlkFzaPW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f52dc8ae-cbad-485c-c9fc-1d83a982b2ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor video_path in all_videos:\\n    for segment in preprocess_and_segment_video(video_path):\\n        print(f\"Segment shape: {segment.shape} : {video_path}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "def can_read_file(file_path):\n",
        "    \"\"\"Attempt to open and read a small portion of the file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            file.read(1)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Unable to read file {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def preprocess_and_segment_video(video_path, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Generator to preprocess raw videos by resizing, converting color space, and normalization.\n",
        "    Includes checks for common file access issues.\n",
        "    \"\"\"\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"File does not exist: {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Check if the file can be read (covers permissions and potentially other read-related errors)\n",
        "    if not can_read_file(video_path):\n",
        "        # can_read_file will print the specific error\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise IOError(f\"OpenCV could not open video file for an unknown reason: {video_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    frames_count = 0\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            # Check if the frame is actually non-empty to avoid processing empty arrays\n",
        "            if frame.size == 0:\n",
        "                print(\"Encountered an empty frame in the video.\")\n",
        "                continue  # Skip this frame and move to the next\n",
        "            frame = cv2.resize(frame, resize_shape)\n",
        "            frame = frame.astype(np.float32) / 255.0\n",
        "            frames.append(frame)\n",
        "            frames_count += 1\n",
        "            if frames_count == segment_length:\n",
        "                yield np.stack(frames, axis=0)\n",
        "                frames_count = 0\n",
        "                frames.clear()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_path}: {e}\")\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    if frames_count > 0:\n",
        "        while frames_count < segment_length:\n",
        "            frames.append(np.zeros(resize_shape + (3,), dtype=np.float32))\n",
        "            frames_count += 1\n",
        "        yield np.stack(frames, axis=0)\n",
        "\n",
        "\n",
        "all_videos = anomaly_videos + normal_videos\n",
        "\"\"\"\n",
        "for video_path in all_videos:\n",
        "    for segment in preprocess_and_segment_video(video_path):\n",
        "        print(f\"Segment shape: {segment.shape} : {video_path}\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIjfSbVTWdsZ"
      },
      "source": [
        "#Video Processing Utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_video_files(directory, extension='mp4'):\n",
        "    \"\"\"\n",
        "    List all video files in the given directory with the specified extension.\n",
        "\n",
        "    Parameters:\n",
        "    - directory: Path to the directory to search for video files.\n",
        "    - extension: Extension of the video files to search for (default is '*.mp4').\n",
        "\n",
        "    Returns:\n",
        "    - A list of paths to the video files found.\n",
        "    \"\"\"\n",
        "     # Check if the directory exists\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Error: Directory does not exist - {directory}\")\n",
        "        return []  # Return an empty list instead of None\n",
        "\n",
        "    # Check if the directory can be accessed (read permissions)\n",
        "    if not os.access(directory, os.R_OK):\n",
        "        print(f\"Error: Directory cannot be read, check permissions - {directory}\")\n",
        "        return []  # Return an empty list instead of None\n",
        "\n",
        "    # Perform a recursive search for video files\n",
        "    pattern = os.path.join(directory, '**', f'*.{extension}')\n",
        "    _video_files = glob.glob(pattern, recursive=True)\n",
        "    return _video_files\n",
        "\n",
        "\n",
        "def shuffle_together(video_paths, labels):\n",
        "    \"\"\"\n",
        "    Shuffle two lists in unison.\n",
        "\n",
        "    Parameters:\n",
        "    - video_paths: The first list to shuffle.\n",
        "    - label: The second list to shuffle, must be the same length as list1.\n",
        "\n",
        "    Returns:\n",
        "    - The shuffled video_paths and label.\n",
        "    \"\"\"\n",
        "    if len(video_paths) != len(labels):\n",
        "        raise ValueError(\"The lists to be shuffled must be the same length.\")\n",
        "\n",
        "    # sklearn's shuffle function to shuffle both lists in unison\n",
        "    video_paths_shuffled, label_shuffled = shuffle(video_paths, labels)\n",
        "    return video_paths_shuffled, label_shuffled\n",
        "\n",
        "def get_total_frames(video_path):\n",
        "    \"\"\"\n",
        "    Returns the total number of frames in a video.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: The path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - The total number of frames as an integer.\n",
        "    \"\"\"\n",
        "    # Initialize the video capture object with the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Failed to open video file: {video_path}\")\n",
        "        return 0  # Indicates that the video could not be opened\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    return total_frames\n",
        "\n",
        "def segments_per_video(video_path, segment_length=32):\n",
        "    total_frames = get_total_frames(video_path)  # This function needs to return the total frame count\n",
        "    segments = 0\n",
        "    if total_frames != 0:\n",
        "          segments = total_frames // segment_length\n",
        "          if total_frames % segment_length != 0:\n",
        "              segments += 1  # Account for the last, potentially shorter, segment\n",
        "    print(\"{} frames, {} segments for {}\".format(total_frames,segments,video_path))\n",
        "    return segments\n",
        "\"\"\"\n",
        "video_path = anomaly_videos[0]\n",
        "total_segments = segments_per_video(video_path, segment_length=32)\n",
        "print(f\"Total segments: {total_segments}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T79crLju9ZIA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8b1149e2-1fd0-4543-fc55-860cb9e2f191"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nvideo_path = anomaly_videos[0]\\ntotal_segments = segments_per_video(video_path, segment_length=32)\\nprint(f\"Total segments: {total_segments}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Features and Labels"
      ],
      "metadata": {
        "id": "oh0-78i09aJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4p6erhh4zIII"
      },
      "outputs": [],
      "source": [
        "def create_dataset_from_videos(video_paths, labels, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Creates a TensorFlow dataset of video segments with corresponding labels.\n",
        "\n",
        "    Args:\n",
        "        video_paths (list of str): Paths to video files.\n",
        "        labels (list of int): Labels for each video file.\n",
        "        resize_shape (tuple): The target shape for resizing frames.\n",
        "        segment_length (int): Number of frames per video segment.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A dataset of video segments and labels.\n",
        "    \"\"\"\n",
        "    def generator():\n",
        "        count = 0;\n",
        "        for video_path, label in zip(video_paths, labels):\n",
        "            count = count + 1;\n",
        "            video_gen = preprocess_and_segment_video(video_path, resize_shape, segment_length)\n",
        "            print(\"Video files count: {} processing: {}\".format(count, video_path))\n",
        "            # Iterate over the generator and yield its items\n",
        "            for segment in video_gen:  # Iterate over items yielded by video_gen\n",
        "                yield segment, label\n",
        "            \"\"\"\n",
        "            if features is not None:\n",
        "                # Yield features and label if processing was successful\n",
        "                yield features, label\n",
        "            else:\n",
        "                print(f\"Skipping video {video_path}, unable to process.\")\n",
        "            \"\"\"\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(segment_length, *resize_shape, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4B-y1mRl1Pm"
      },
      "source": [
        "# Fine-Tune Pretrained Video Swin Transformer Model for Video Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Yzq9StfOfWfv"
      },
      "outputs": [],
      "source": [
        "def build_finetune_model():\n",
        "    \"\"\"\n",
        "    Fine-tune pretrained model.\n",
        "\n",
        "    Parameters:\n",
        "        saved_model_path (str): Path to the saved Video Swin Transformer model.\n",
        "\n",
        "    Returns:\n",
        "        model (keras.Model): Fine-tuned model for shooting classification.\n",
        "    \"\"\"\n",
        "    # Load the pretrained Video Swin model\n",
        "    video_swin = load_model(saved_model_path, compile=False)\n",
        "\n",
        "    # Fine-tuning configuration: set the last N layers to be trainable\n",
        "    # N=1, fine-tune the last layer of the pretrained model\n",
        "    \"\"\"\n",
        "    N=1\n",
        "    for layer in video_swin.layers[:-N]:\n",
        "        layer.trainable = False\n",
        "    for layer in video_swin.layers[-N:]:\n",
        "        layer.trainable = True\n",
        "    \"\"\"\n",
        "    # Set the entire Video Swin model to non-trainable\n",
        "    video_swin.trainable = False\n",
        "\n",
        "    # Downstream model for binary classification\n",
        "    model = Sequential([\n",
        "        video_swin,\n",
        "        # Assume video_swin's output shape is compatible with the Dense layer's input\n",
        "        Dense(512, activation='relu', kernel_regularizer=regularizers.l1(1e-5)),  # Apply L1 regularization\n",
        "        Dropout(0.6),\n",
        "        Dense(32, activation='relu', kernel_regularizer=regularizers.l1(1e-5)),  # Apply L1 regularization again if needed\n",
        "        Dropout(0.6),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7RHFSSomcmd"
      },
      "source": [
        "#Train the Fine-Tuned Model for Shooting Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rL-zPaFnqLq5"
      },
      "outputs": [],
      "source": [
        "# Define the base directory to save checkpoints\n",
        "model_chkpt_filename = f\"training_1/vad_{int(time.time())}\"\n",
        "checkpoint_path = os.path.join(base_directory, model_chkpt_filename)\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create the base directory if it doesn't exist\n",
        "os.makedirs(base_directory, exist_ok=True)\n",
        "\n",
        "# Callbacks configuration\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss'),  # Save the best model based on val_loss\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10),  # Stop training when `val_loss` is no longer improving\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.1,\n",
        "        patience=5),  # Reduce learning rate when `val_loss` plateaus\n",
        "    TensorBoard(\n",
        "        log_dir=os.path.join(base_directory, 'logs'),  # Path to save log files for TensorBoard in Google Drive\n",
        "        histogram_freq=1,  # Record activation histograms every 1 epoch\n",
        "        embeddings_freq=1)  # Record embedding data every 1 epoch\n",
        "]\n",
        "\n",
        "\n",
        "# Define a custom callback for clearing the session and collecting garbage\n",
        "class ClearSessionCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        tf.keras.backend.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "# Assume `callbacks_list` is already defined. Add the new callback to it.\n",
        "callbacks_list.append(ClearSessionCallback())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Instance Learning Loss"
      ],
      "metadata": {
        "id": "dXmpOUVlqyPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mil_ranking_loss(sparsity_weight=0.01, smoothness_weight=0.01, margin=5.0):\n",
        "    \"\"\"\n",
        "    MIL ranking loss with sparsity and smoothness constraints.\n",
        "\n",
        "    Args:\n",
        "    - sparsity_weight: Weight for the sparsity term.\n",
        "    - smoothness_weight: Weight for the smoothness term.\n",
        "    - margin: Margin for the ranking loss.\n",
        "\n",
        "    Returns:\n",
        "    - A loss function that takes (y_true, y_pred) as inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        # Separate the anomaly scores into positive and negative samples based on the labels.\n",
        "        positive_scores = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        negative_scores = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "\n",
        "        # Calculate the highest scores for positive and negative samples.\n",
        "        highest_positive_score = tf.reduce_max(positive_scores)\n",
        "        highest_negative_score = tf.reduce_max(negative_scores)\n",
        "\n",
        "        # Calculate ranking loss.\n",
        "        ranking_loss = tf.maximum(0.0, margin - highest_positive_score + highest_negative_score)\n",
        "\n",
        "        # Calculate sparsity loss (L1 norm of the predictions).\n",
        "        sparsity_loss = tf.reduce_sum(tf.abs(y_pred))\n",
        "\n",
        "        # Calculate smoothness loss (squared difference between adjacent anomaly scores).\n",
        "        diffs = y_pred[:, 1:] - y_pred[:, :-1]\n",
        "        smoothness_loss = tf.reduce_sum(tf.square(diffs))\n",
        "\n",
        "        # Combine the losses using the weights and loss components.\n",
        "        total_loss = ranking_loss + (sparsity_weight * sparsity_loss) + (smoothness_weight * smoothness_loss)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ra-kgXNIq8Le"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anomaly_videos, normal_videos = list_video_files(anomaly_dir), list_video_files(normal_dir)\n",
        "print(\"\\nanomaly_videos\", len(anomaly_videos))\n",
        "print(\"\\nnormal_videos\", len(normal_videos))\n",
        "\n",
        "video_paths = anomaly_videos + normal_videos\n",
        "labels = [1] * len(anomaly_videos) + [0] * len(normal_videos)\n",
        "print(\"\\nvideo paths {} and labels {}\".format(video_paths,labels))\n",
        "video_paths, labels = shuffle_together(video_paths, labels)\n",
        "print(\"\\nShuffled: video paths {} and labels {}\".format(video_paths,labels))\n",
        "dataset = create_dataset_from_videos(video_paths, labels)\n",
        "\n",
        "\n",
        "cache_data_filename = f\"./cache.tf-data_{int(time.time())}\"\n",
        "dataset = dataset.cache(cache_data_filename)\n",
        "\n",
        "\"\"\"\n",
        "for features, label in dataset.take(3):\n",
        "    print(\"\\nFeatures shape:\", features.numpy().shape)\n",
        "\"\"\"\n",
        "\n",
        "# calculate the total video segments i.e training dataset size\n",
        "total_segments_actual = sum(segments_per_video(video_path, segment_length=32) for video_path in video_paths)\n",
        "print(f\"\\nTotal segments across all videos: {total_segments_actual}\")\n",
        "\n",
        "\"\"\"\n",
        "total_segments_actual = sum(1 for _ in dataset)\n",
        "print(\"\\nTotal segments actually in dataset:\", total_segments_actual)\n",
        "\"\"\"\n",
        "# Calculate split sizes\n",
        "train_data_size = int(total_segments_actual * 0.8)\n",
        "print(\"\\nTraining size split into :\", train_data_size)\n",
        "\n",
        "\n",
        "val_data_size = total_segments_actual - train_data_size\n",
        "\n",
        "print(\"\\nValidation size split into :\", val_data_size)\n",
        "\n",
        "\n",
        "# Ensure dataset is shuffled (use actual total size if known for better shuffling, uses more memory)\n",
        "dataset = dataset.shuffle(buffer_size=(total_segments_actual//2),reshuffle_each_iteration=True)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "train_segments_split = sum(1 for _ in train_dataset)\n",
        "print(\"\\nTraining segments split:\", train_segments_split)\n",
        "\n",
        "val_segments_split = sum(1 for _ in val_dataset)\n",
        "print(\"\\nValidation segments split:\", val_segments_split)\n",
        "\n",
        "for features, label in train_dataset.take(1):\n",
        "    print(\"Train: Features shape:\", features.numpy().shape)\n",
        "    print(\"Train: Label shape:\", label.numpy())\n",
        "    print(\"\\nTrain: Labels:\", label)\n",
        "\n",
        "for features, label in val_dataset.take(1):\n",
        "    print(\"\\nVal: Features shape:\", features.numpy().shape)\n",
        "    print(\"val: Label shape:\", label.numpy())\n",
        "    print(\"val: Labels:\", label)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "xfMMTEpcptbT",
        "outputId": "337d2912-a317-45ae-afca-8569b1734b75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "anomaly_videos 2\n",
            "\n",
            "normal_videos 2\n",
            "\n",
            "video paths ['/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting001_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting032_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos001_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos061_x264.mp4'] and labels [1, 1, 0, 0]\n",
            "\n",
            "Shuffled: video paths ['/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting032_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos061_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting001_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos001_x264.mp4'] and labels [1, 0, 1, 0]\n",
            "21681 frames, 678 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting032_x264.mp4\n",
            "17693 frames, 553 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos061_x264.mp4\n",
            "253 frames, 8 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting001_x264.mp4\n",
            "544 frames, 17 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos001_x264.mp4\n",
            "\n",
            "Total segments across all videos: 1256\n",
            "\n",
            "Training size split into : 1004\n",
            "\n",
            "Validation size split into : 252\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_segments_split = sum(1 for _ in train_dataset)\\nprint(\"\\nTraining segments split:\", train_segments_split)\\n\\nval_segments_split = sum(1 for _ in val_dataset)\\nprint(\"\\nValidation segments split:\", val_segments_split)\\n\\nfor features, label in train_dataset.take(1):\\n    print(\"Train: Features shape:\", features.numpy().shape)\\n    print(\"Train: Label shape:\", label.numpy())\\n    print(\"\\nTrain: Labels:\", label)\\n\\nfor features, label in val_dataset.take(1):\\n    print(\"\\nVal: Features shape:\", features.numpy().shape)\\n    print(\"val: Label shape:\", label.numpy())\\n    print(\"val: Labels:\", label)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_correct_dimensions(video_data, labels):\n",
        "    # Now the function directly accepts video_data and labels as separate arguments\n",
        "\n",
        "    # Define the expected shape, excluding batch size to allow dynamic sizes\n",
        "    expected_shape = (32, 224, 224, 3)  # Example: (segments, height, width, channels)\n",
        "\n",
        "    # Ensure the shape of the video data matches the expected shape, excluding the batch size\n",
        "    tf.debugging.assert_equal(tf.shape(video_data)[1:], expected_shape, message=\"Video batch dimensions are incorrect\")\n",
        "\n",
        "    # Return video_data and labels unchanged\n",
        "    return video_data, labels\n"
      ],
      "metadata": {
        "id": "rP3ErVp2ik1q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUKmgvFm43KK",
        "outputId": "8fdcbf73-8a26-4933-b779-5666d8342c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_steps_per_epoch 125\n",
            "validation_steps_per_epoch 31\n",
            "Epoch 1/10\n",
            "Video files count: 1 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting032_x264.mp4\n",
            "Video files count: 2 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos061_x264.mp4\n",
            "Video files count: 3 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting001_x264.mp4\n",
            "Video files count: 4 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos001_x264.mp4\n",
            " 38/125 [========>.....................] - ETA: 58:50 - loss: 4.5435 - accuracy: 0.6974 - precision: 0.8527 - recall: 0.6011 - auc: 0.8326"
          ]
        }
      ],
      "source": [
        "# Batch for memory efficiency\n",
        "batch_size = 8\n",
        "num_epochs = 10\n",
        "\n",
        "train_dataset = dataset.take(train_data_size)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=train_data_size).repeat(num_epochs).batch(batch_size)\n",
        "cache_train_filename = f\"./cache.tf-train_{int(time.time())}\"\n",
        "dataset = dataset.cache(cache_train_filename)\n",
        "\n",
        "val_dataset = dataset.skip(val_data_size)\n",
        "val_dataset = val_dataset.shuffle(buffer_size=val_data_size).repeat(num_epochs).batch(batch_size)\n",
        "\n",
        "\n",
        "val_dataset = train_dataset.cache(filename=\"./cache.tf-val\")\n",
        "cache_val_filename = f\"./cache.tf-val_{int(time.time())}\"\n",
        "dataset = dataset.cache(cache_val_filename)\n",
        "\n",
        "train_dataset = train_dataset.map(ensure_correct_dimensions)\n",
        "val_dataset = val_dataset.map(ensure_correct_dimensions)\n",
        "\n",
        "\"\"\"\n",
        "for features, label in train_dataset.take(1):\n",
        "    print(\"Train: Labels:\", label.numpy())\n",
        "    print(\"Train: Features shape:\", features.numpy().shape)\n",
        "\n",
        "for features, label in val_dataset.take(1):\n",
        "    print(\"Val: Features shape:\", features.numpy().shape)\n",
        "    print(\"val: Labels:\", label.numpy())\n",
        "\"\"\"\n",
        "\n",
        "training_steps_per_epoch = train_data_size // batch_size\n",
        "validation_steps_per_epoch = val_data_size // batch_size\n",
        "\n",
        "print(\"training_steps_per_epoch\",training_steps_per_epoch)\n",
        "print(\"validation_steps_per_epoch\",validation_steps_per_epoch)\n",
        "\n",
        "\n",
        "# Asynchronously fetch batches while model is training\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Configure the Adagrad optimizer\n",
        "adagrad = Adagrad(learning_rate=0.01, epsilon=1e-08)\n",
        "\n",
        "finetune_model = build_finetune_model()\n",
        "# Compile model with MIL loss, adam optimizer and metrics\n",
        "finetune_model.compile(\n",
        "    optimizer=adagrad,\n",
        "    loss=mil_ranking_loss(sparsity_weight=0.01, smoothness_weight=0.01),\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        Precision(name='precision'),\n",
        "        Recall(name='recall'),\n",
        "        AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "for features, labels in train_dataset.take(1):\n",
        "    print(features.shape)\n",
        "\n",
        "for features, labels in val_dataset.take(1):\n",
        "    print(features.shape)\n",
        "\"\"\"\n",
        "# Train the model\n",
        "history = finetune_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=training_steps_per_epoch,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=validation_steps_per_epoch,  # Add this line\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ced1WK7dnwdb"
      },
      "source": [
        "# Evaluate Final Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VI6xmkh497Z"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot precision, recall, and AUC\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Precision\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['precision'], label='Train Precision')\n",
        "plt.plot(history.history['val_precision'], label='Validation Precision')\n",
        "plt.title('Precision over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "\n",
        "# Recall\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['recall'], label='Train Recall')\n",
        "plt.plot(history.history['val_recall'], label='Validation Recall')\n",
        "plt.title('Recall over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['auc'], label='Train AUC')\n",
        "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "plt.title('AUC over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "#test_loss, test_accuracy, test_precision, test_recall, test_auc = finetune_model.evaluate(test_dataset)\n",
        "#print(f\"Test Metrics:\\n Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, AUC: {test_auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Fine-Tuned Model\n"
      ],
      "metadata": {
        "id": "5ViGgioyst_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your training file\n",
        "test_file_path = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/UCF_Crimes-Train-Test-Split/Anomaly_Detection_splits/Anomaly_Test.txt'\n",
        "\n",
        "\n",
        "# Read video paths from the file\n",
        "video_paths = read_video_paths_from_file(test_file_path,base_dataset_dir)\n",
        "\n",
        "# Assuming the directory names \"Anomaly\" and \"Normal\" are part of the paths to distinguish the types\n",
        "anomaly_videos = filter_video_paths(video_paths, 'Shooting')\n",
        "normal_videos = filter_video_paths(video_paths, 'Normal')[:27]\n",
        "\n",
        "for anomaly_path in anomaly_videos:\n",
        "    print(anomaly_path)\n",
        "\n",
        "for normal_path in normal_videos:\n",
        "    print(normal_path)\n",
        "\n",
        "# Preprocess the video\n",
        "test_video_data = preprocess_and_segment_video(anomaly_videos[0], (224,224), 32)\n",
        "\n",
        "test_video_data = np.expand_dims(test_video_data, axis=0)  # Add batch dimension\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = tf.keras.models.load_model(checkpoint_path)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(test_video_data)\n",
        "\n",
        "# Define an anomaly threshold\n",
        "anomaly_threshold = 0.5  # This is arbitrary; adjust based on your model and use case\n",
        "\n",
        "# Check if any segment is predicted as anomalous\n",
        "is_anomalous = np.any(predictions > anomaly_threshold)\n",
        "\n",
        "print(f\"Video is {'anomalous' if is_anomalous else 'normal'}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sHPhqV4us1U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydJFOP4CMdhA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1gynqv509-foIt7uHod2zNMODzUkFN7MN",
      "authorship_tag": "ABX9TyMBrShKpFY2nzdR7Et1zvUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}