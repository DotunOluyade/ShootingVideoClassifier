{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DotunOluyade/ShootingVideoClassifier/blob/main/VideoAnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqG2VmTbSxG7"
      },
      "source": [
        "\n",
        "# Real-Time Crime Prevention using Video Anomaly Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014sqOa1E1R_"
      },
      "source": [
        "Comparative Analysis of Pretrained Video Classification models in detecting real-time shooting crime for peace and safety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAZ2GqfATJYM"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "A sub-set of UCF-Crime dataset is used for this research work. It contains 128 hours of video, comprising 1900 long untrimmed real world surveillance videos, with 13 realistic anomalies as well as normal activities (Sultani et al., *2018*).\n",
        "\n",
        "This research uses the shooting dataset only to fine-tune selected video classification pretrained models, compares and evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uparbOrXN11d"
      },
      "source": [
        "### Specify Dataset Location\n",
        "\n",
        "Import package dependencies and define variables for datasets and pretrained model.\n",
        "\n",
        "Video Swim a pure transformer based video modeling algorithm with its pretrained model is used for feature extraction and fine-tuned with the shooting dataset for classifying videos as shooting or non shooting videos (Liu et al.,2022).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "seFZ6F6PTF71"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle as sk_shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "import os\n",
        "import glob\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "\n",
        "base_directory = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/'\n",
        "normal_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Normal_Videos'\n",
        "anomaly_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Shooting'\n",
        "anomaly_text_file = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Anomaly_Train.txt'\n",
        "anomaly_train = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/UCF_Crimes-Train-Test-Split/Anomaly_Detection_splits/Anomaly_Train.txt'\n",
        "\n",
        "saved_model_path = '/content/drive/MyDrive/VideoAnomalyDetection/pretrained/Video Swin Transformer/TFVideoSwinB_K600_IN22K_P244_W877_32x224'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QH1U4AMYe3c"
      },
      "source": [
        "# Descriptive Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "resources": {
            "http://localhost:8080/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse001_x264.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse002_x264.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse003_x264.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse004_x264.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse005_x264.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse006_x264.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            }
          }
        },
        "id": "cUA6fmhdZcf0",
        "outputId": "79449732-31f5-4ede-879f-dcd64c6df529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training videos:                                   Record\n",
            "count                              1610\n",
            "unique                             1610\n",
            "top     Vandalism/Vandalism050_x264.mp4\n",
            "freq                                  1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='display: flex; flex-wrap: wrap;'>\n",
              "    <div style='flex: 33%; padding: 5px;'>\n",
              "        <video width=\"100%\" controls>\n",
              "          <source src='/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse001_x264.mp4' type=\"video/mp4\">\n",
              "          Your browser does not support the video tag.\n",
              "        </video>\n",
              "    </div>\n",
              "    \n",
              "    <div style='flex: 33%; padding: 5px;'>\n",
              "        <video width=\"100%\" controls>\n",
              "          <source src='/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse002_x264.mp4' type=\"video/mp4\">\n",
              "          Your browser does not support the video tag.\n",
              "        </video>\n",
              "    </div>\n",
              "    \n",
              "    <div style='flex: 33%; padding: 5px;'>\n",
              "        <video width=\"100%\" controls>\n",
              "          <source src='/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse003_x264.mp4' type=\"video/mp4\">\n",
              "          Your browser does not support the video tag.\n",
              "        </video>\n",
              "    </div>\n",
              "    \n",
              "    <div style='flex: 33%; padding: 5px;'>\n",
              "        <video width=\"100%\" controls>\n",
              "          <source src='/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse004_x264.mp4' type=\"video/mp4\">\n",
              "          Your browser does not support the video tag.\n",
              "        </video>\n",
              "    </div>\n",
              "    \n",
              "    <div style='flex: 33%; padding: 5px;'>\n",
              "        <video width=\"100%\" controls>\n",
              "          <source src='/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse005_x264.mp4' type=\"video/mp4\">\n",
              "          Your browser does not support the video tag.\n",
              "        </video>\n",
              "    </div>\n",
              "    \n",
              "    <div style='flex: 33%; padding: 5px;'>\n",
              "        <video width=\"100%\" controls>\n",
              "          <source src='/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Abuse/Abuse006_x264.mp4' type=\"video/mp4\">\n",
              "          Your browser does not support the video tag.\n",
              "        </video>\n",
              "    </div>\n",
              "    </div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_Normal_Videos_Anomaly    800\n",
            "Robbery                           145\n",
            "RoadAccidents                     127\n",
            "Stealing                           95\n",
            "Burglary                           87\n",
            "Abuse                              48\n",
            "Assault                            47\n",
            "Vandalism                          45\n",
            "Arrest                             45\n",
            "Fighting                           45\n",
            "Arson                              41\n",
            "Explosion                          29\n",
            "Shoplifting                        29\n",
            "Shooting                           27\n",
            "Name: Record, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "training_data = pd.read_csv(anomaly_train, header=None, names=['Record'])\n",
        "\n",
        "\n",
        "#shooting_videos = data[data['Record'].str.contains('Shooting')]\n",
        "#normal_videos = data[data['Record'].str.contains('Training_Normal_Videos_Anomaly')]\n",
        "\n",
        "#print(\"\\nShooting videos: \", shooting_videos.describe())\n",
        "#print(\"\\nNormal videos: \", normal_videos.describe())\n",
        "print(\"\\nTraining videos: \", training_data.describe())\n",
        "\n",
        "\n",
        "# Construct the absolute paths\n",
        "training_data['Absolute_Path'] = base_directory + training_data['Record']\n",
        "\n",
        "\n",
        "# Placeholder list to hold video HTML strings\n",
        "videos_html = []\n",
        "\n",
        "# Loop through the DataFrame and create video HTML strings\n",
        "for video_path in training_data['Absolute_Path'].head(6):\n",
        "    # Convert your Google Drive shared link to direct link here\n",
        "    # For demonstration, using a direct link format directly\n",
        "    direct_link = video_path.replace(\"/view?usp=sharing\", \"\").replace(\"https://drive.google.com/file/d/\", \"https://drive.google.com/uc?export=view&id=\")\n",
        "\n",
        "    video_html = f\"\"\"\n",
        "    <div style='flex: 33%; padding: 5px;'>\n",
        "        <video width=\"100%\" controls>\n",
        "          <source src='{direct_link}' type=\"video/mp4\">\n",
        "          Your browser does not support the video tag.\n",
        "        </video>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    videos_html.append(video_html)\n",
        "\n",
        "# Wrap the videos in a flex container for a 3-column grid\n",
        "grid_html = f\"<div style='display: flex; flex-wrap: wrap;'>{''.join(videos_html)}</div>\"\n",
        "\n",
        "# Display the grid\n",
        "display(HTML(grid_html))\n",
        "\n",
        "\n",
        "\n",
        "# Split the 'Record' column on '/', take the first part, and perform a value count\n",
        "category_counts = training_data['Record'].str.split('/').str[0].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(category_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pm7z6-3UOec"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Resize, normalize and split videos into frames. Batch frames in 32 segments.\n",
        "\n",
        "Use data generators to prevent loading large datset into memory at once, to better utilize memory for resource constrained environement.\n",
        "\n",
        "Generator loads and preprocess 32 frames at each interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "77GImlkFzaPW"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_segment_video(video_path, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Preprocess raw videos by resizing, color space conversion, normalization\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: absolute path to video file\n",
        "    - resize_shape: shape (224,224) to resize video to\n",
        "    - segment_length: value to segment frames of video to\n",
        "\n",
        "    Returns:\n",
        "    - a batch of 32 preprocessed frames of shape (32, 224, 224, 3), using generators\n",
        "    \"\"\"\n",
        "    def video_generator():\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break  # Exit loop if video ends\n",
        "            frame = cv2.resize(frame, resize_shape)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255.0\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == segment_length:\n",
        "                yield np.array(frames)\n",
        "                frames = []\n",
        "\n",
        "        # Handle the last segment if it has fewer frames than segment_length\n",
        "        if frames:  # Check if there are remaining frames\n",
        "            # Pad the last segment to have 'segment_length' frames\n",
        "            padding = [np.zeros_like(frames[0]) for _ in range(segment_length - len(frames))]\n",
        "            frames.extend(padding)\n",
        "            yield np.array(frames)  # Yield the padded last segment\n",
        "        cap.release()\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        video_generator,\n",
        "        output_signature=tf.TensorSpec(shape=(segment_length, *resize_shape, 3), dtype=tf.float32)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIjfSbVTWdsZ"
      },
      "source": [
        "### List and Shuffle Video files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p6erhh4zIII",
        "outputId": "b495fff6-f53b-40f2-bea4-c01047a437ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anomaly_videos 30\n",
            "normal_videos 27\n",
            "Total segments:  172\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Note: Changed 'train_size' argument to 'train_size_ratio' to avoid confusion with the calculated 'train_size' variable.\n",
        "\"\"\"\n",
        "def list_video_files(directory, extension='*.mp4'):\n",
        "    \"\"\"\n",
        "    List all video files in the given directory with the specified extension.\n",
        "\n",
        "    Parameters:\n",
        "    - directory: Path to the directory to search for video files.\n",
        "    - extension: Extension of the video files to search for (default is '*.mp4').\n",
        "\n",
        "    Returns:\n",
        "    - A list of paths to the video files found.\n",
        "    \"\"\"\n",
        "    # Create a full path pattern by joining directory and extension\n",
        "    pattern = os.path.join(directory, extension)\n",
        "    # Use glob to find all files matching the pattern\n",
        "    video_files = glob.glob(pattern)\n",
        "\n",
        "    return video_files\n",
        "\n",
        "\n",
        "def shuffle_together(video_paths, label):\n",
        "    \"\"\"\n",
        "    Shuffle two lists in unison.\n",
        "\n",
        "    Parameters:\n",
        "    - video_paths: The first list to shuffle.\n",
        "    - label: The second list to shuffle, must be the same length as list1.\n",
        "\n",
        "    Returns:\n",
        "    - The shuffled video_paths and label.\n",
        "    \"\"\"\n",
        "    if len(video_paths) != len(label):\n",
        "        raise ValueError(\"The lists to be shuffled must be the same length.\")\n",
        "\n",
        "    # sklearn's shuffle function to shuffle both lists in unison\n",
        "    video_paths_shuffled, label_shuffled = shuffle(video_paths, label)\n",
        "    return video_paths_shuffled, label_shuffled\n",
        "\n",
        "\n",
        "def create_dataset_from_videos(video_paths, labels, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Creates a TensorFlow dataset of video segments with corresponding labels.\n",
        "\n",
        "    Args:\n",
        "        video_paths (list of str): Paths to video files.\n",
        "        labels (list of int): Labels for each video file.\n",
        "        resize_shape (tuple): The target shape for resizing frames.\n",
        "        segment_length (int): Number of frames per video segment.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A dataset of video segments and labels.\n",
        "    \"\"\"\n",
        "    def generator():\n",
        "        for video_path, label in zip(video_paths, labels):\n",
        "            # Use preprocess_and_segment_video to yield segments\n",
        "            for segment in preprocess_and_segment_video(video_path, resize_shape, segment_length):\n",
        "                #visualize_preprocess_and_segment_video(segment,resize_shape,label,segment_length)\n",
        "                yield segment.numpy(), label\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(segment_length, *resize_shape, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_video_duration(video_path):\n",
        "    \"\"\"\n",
        "    Get the duration of a video in seconds.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: The file path to the video.\n",
        "\n",
        "    Returns:\n",
        "    - The duration of the video in seconds as a float.\n",
        "    \"\"\"\n",
        "    # Initialize the video capture object\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Get the frame rate of the video\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Calculate the duration\n",
        "    duration = total_frames / fps if fps > 0 else 0\n",
        "\n",
        "    return duration\n",
        "\n",
        "\n",
        "def calculate_total_segments(video_paths, segment_length=32):\n",
        "    total_segments = 0\n",
        "    for video_path in video_paths:\n",
        "        duration = get_video_duration(video_path)  # This function needs to be defined\n",
        "        segments = duration // segment_length\n",
        "        # If the video is not perfectly divisible by segment_length, consider the remainder as an additional segment\n",
        "        if duration % segment_length != 0:\n",
        "            segments += 1\n",
        "        total_segments += segments\n",
        "    return total_segments\n",
        "\n",
        "\n",
        "\n",
        "# Assuming implementations for list_video_files and shuffle_together\n",
        "anomaly_videos, normal_videos = list_video_files(anomaly_dir), list_video_files(normal_dir)\n",
        "print(\"anomaly_videos\", len(anomaly_videos))\n",
        "print(\"normal_videos\", len(normal_videos))\n",
        "\n",
        "video_paths = anomaly_videos + normal_videos\n",
        "labels = [1] * len(anomaly_videos) + [0] * len(normal_videos)\n",
        "\n",
        "video_paths, labels = shuffle_together(video_paths, labels)\n",
        "\n",
        "dataset = create_dataset_from_videos(video_paths, labels)\n",
        "\n",
        "# calculate the total video segments\n",
        "total_segments = int(calculate_total_segments(video_paths, segment_length=32))\n",
        "\n",
        "print(\"Total segments: \", total_segments)\n",
        "\n",
        "# Calculate split sizes\n",
        "train_size = int(total_segments * 0.8)\n",
        "val_size = total_segments - train_size  # Optionally, to ensure the entire dataset is used\n",
        "\n",
        "# Ensure dataset is shuffled if needed (use actual total size if known for better shuffling)\n",
        "# Note: This step requires substantial memory for large datasets; adjust buffer size as feasible\n",
        "dataset = dataset.shuffle(buffer_size=total_segments)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "\n",
        "def test_list_video_files():\n",
        "    # Create a temporary directory\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        # Create dummy files\n",
        "        video_filenames = ['video1.mp4', 'video2.mp4']\n",
        "        non_video_filenames = ['image1.png', 'text1.txt']\n",
        "        for filename in video_filenames + non_video_filenames:\n",
        "            open(os.path.join(temp_dir, filename), 'a').close()\n",
        "\n",
        "        # Call the function under test\n",
        "        listed_files = list_video_files(temp_dir, '*.mp4')\n",
        "\n",
        "\n",
        "        # Assert correctness\n",
        "        assert len(listed_files) == len(video_filenames), \"Incorrect number of video files listed.\"\n",
        "        assert all(os.path.basename(file) in video_filenames for file in listed_files), \"Listed files are not correct.\"\n",
        "\n",
        "# You would call this test function in your test suite\n",
        "test_list_video_files()\n"
      ],
      "metadata": {
        "id": "A3G7jchi208X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4B-y1mRl1Pm"
      },
      "source": [
        "# Fine-Tune A Pretrained Video Swin Transformer Model for Video Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Yzq9StfOfWfv"
      },
      "outputs": [],
      "source": [
        "def build_finetune_model():\n",
        "    \"\"\"\n",
        "    Fine tune pretrained model\n",
        "\n",
        "    Returns:\n",
        "        model (Fine-tuned for shooting classification)\n",
        "    \"\"\"\n",
        "   # import pretrained model, i.e.\n",
        "    video_swin = keras.models.load_model(saved_model_path, compile= False)\n",
        "    video_swin.trainable = False\n",
        "\n",
        "    # downstream model\n",
        "    model = keras.Sequential([\n",
        "        video_swin,\n",
        "        # video_swin outputs a 1D tensor of shape (600,)\n",
        "        layers.Dense(1024, activation='relu'),  # Additional Dense layer\n",
        "        layers.Dropout(0.5),  # Dropout layer to reduce overfitting\n",
        "        layers.Dense(1, activation='sigmoid')  # Final Dense layer for binary classification\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7RHFSSomcmd"
      },
      "source": [
        "#Train the Fine-Tuned Model for Shooting Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rL-zPaFnqLq5"
      },
      "outputs": [],
      "source": [
        "# Callbacks configuration\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss'),  # Save the best model based on val_loss\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10),  # Stop training when `val_loss` is no longer improving\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.1,\n",
        "        patience=5),  # Reduce learning rate when `val_loss` plateaus\n",
        "    TensorBoard(\n",
        "        log_dir='./logs',  # Path to save log files for TensorBoard\n",
        "        histogram_freq=1,  # Record activation histograms every 1 epoch\n",
        "        embeddings_freq=1)  # Record embedding data every 1 epoch\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "iUKmgvFm43KK",
        "outputId": "e4623fc1-68fe-4a63-b3f2-de6e885305a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x: (1, 32, 224, 224, 3)\n",
            "Shape of y: (1,)\n",
            "Epoch 1/10\n",
            "137/172 [======================>.......] - ETA: 6s - loss: 0.0484 - accuracy: 0.9927 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1720 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6972aee47a80>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m history = finetune_model.fit(train_dataset, epochs=10,steps_per_epoch=steps_per_epoch,\n\u001b[0m\u001b[1;32m     41\u001b[0m validation_data=val_dataset,callbacks=callbacks_list)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1854\u001b[0m                             \u001b[0mpss_evaluation_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pss_evaluation_shards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m                         )\n\u001b[0;32m-> 1856\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1857\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2294\u001b[0m                         ):\n\u001b[1;32m   2295\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2296\u001b[0;31m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[1;32m   2297\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   4106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m         \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Split into training and validation data\n",
        "#train_dataset, val_dataset = create_train_val_datasets(anomaly_dir, normal_dir, train_size=0.8, shuffle_buffer_size=100)\n",
        "\n",
        "# Batch for memory efficiency\n",
        "batch_size = 1\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# Check shapes of the dataset elements after batching\n",
        "for x, y in train_dataset.take(1):\n",
        "    print(\"Shape of x:\", x.shape)\n",
        "    print(\"Shape of y:\", y.shape)\n",
        "\n",
        "steps_per_epoch = total_segments // batch_size\n",
        "\n",
        "\n",
        "# Asynchronously fetch batches while model is training\n",
        "#train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "#val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Only use cache() if memory limits won't be exceeded\n",
        "#train_dataset = train_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "#val_dataset = val_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "finetune_model = build_finetune_model()\n",
        "# Compile model with binary entropy loss, adam optimizer and metrics\n",
        "finetune_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = finetune_model.fit(train_dataset, epochs=10,steps_per_epoch=steps_per_epoch,\n",
        "validation_data=val_dataset,callbacks=callbacks_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ced1WK7dnwdb"
      },
      "source": [
        "# Evaluate Final Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "7VI6xmkh497Z",
        "outputId": "08c38b1c-5d45-4014-fd2c-9373f2eab700"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cce34d2bd6d6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy over epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot precision, recall, and AUC\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Precision\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['precision'], label='Train Precision')\n",
        "plt.plot(history.history['val_precision'], label='Validation Precision')\n",
        "plt.title('Precision over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "\n",
        "# Recall\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['recall'], label='Train Recall')\n",
        "plt.plot(history.history['val_recall'], label='Validation Recall')\n",
        "plt.title('Recall over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['auc'], label='Train AUC')\n",
        "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "plt.title('AUC over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "#test_loss, test_accuracy, test_precision, test_recall, test_auc = finetune_model.evaluate(test_dataset)\n",
        "#print(f\"Test Metrics:\\n Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, AUC: {test_auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq1hW67sMce9"
      },
      "source": [
        "#References:\n",
        "\n",
        "*   Sultani, W., Chen, C. and Shah, M., 2018. Real-world anomaly detection in surveillance videos. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Available at: https://www.crcv.ucf.edu/research/real-world-anomaly-detection-in-surveillance-videos/ 20/03/2024.\n",
        "*   Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L. and Wei, F., 2022. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Available at: https://github.com/innat/VideoSwin 20/03/2024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydJFOP4CMdhA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gynqv509-foIt7uHod2zNMODzUkFN7MN",
      "authorship_tag": "ABX9TyPMTZs45bkJ7eAzv2kWc5lF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}