{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DotunOluyade/ShootingVideoClassifier/blob/main/VideoAnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqG2VmTbSxG7"
      },
      "source": [
        "\n",
        "# Real-Time Crime Prevention using Video Anomaly Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014sqOa1E1R_"
      },
      "source": [
        "**Comparative** Analysis of Pretrained Video Classification models in detecting real-time shooting crime for peace and safety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAZ2GqfATJYM"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "A sub-set of UCF-Crime dataset is used for this research work. It contains 128 hours of video, comprising 1900 long untrimmed real world surveillance videos, with 13 realistic anomalies as well as normal activities (Sultani et al., *2018*).\n",
        "\n",
        "This research uses the shooting dataset only to fine-tune selected video classification pretrained models, compares and evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uparbOrXN11d"
      },
      "source": [
        "### Specify Dataset Location & Import Dependencies\n",
        "\n",
        "Import package dependencies and define variables for datasets and pretrained model.\n",
        "\n",
        "Video Swim a pure transformer based video modeling algorithm with its pretrained model is used for feature extraction and fine-tuned with the shooting dataset for classifying videos as shooting or non shooting videos (Liu et al.,2022).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "seFZ6F6PTF71"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "import cv2\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle as sk_shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "import glob\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "import pandas as pd\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "import gc\n",
        "import random\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Seeding to achieving consistent results across training and inference sessions.\n",
        "\"\"\"\n",
        "\n",
        "# Seed value\n",
        "seed_value = 42\n",
        "\n",
        "# Set python built-in pseudo-random generator\n",
        "random.seed(seed_value)\n",
        "\n",
        "# Set numpy pseudo-random generator\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# Set tensorflow pseudo-random generator\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "# Set PYTHONHASHSEED environment variable\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "\n",
        "base_directory = '/content/drive/MyDrive/VideoAnomalyDetection/'\n",
        "base_dataset_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/'\n",
        "saved_model_path = '/content/drive/MyDrive/VideoAnomalyDetection/pretrained/Video Swin Transformer/TFVideoSwinB_K600_IN22K_P244_W877_32x224'\n",
        "train_file_path = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Anomaly_Train_2.txt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Dataset"
      ],
      "metadata": {
        "id": "DvJzSHoNBfA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_video_paths_from_file(file_path, base_dataset_dir):\n",
        "    full_file_path = os.path.join(base_dataset_dir, file_path)\n",
        "\n",
        "    with open(full_file_path, 'r') as file:\n",
        "        # Concatenate base_dataset_dir with each path read from the file\n",
        "        paths = [os.path.join(base_dataset_dir, line.strip()) for line in file]\n",
        "\n",
        "    return paths\n",
        "\n",
        "def filter_video_paths(paths, filter_keyword):\n",
        "    return [path for path in paths if filter_keyword.lower() in path.lower()]\n",
        "\n",
        "\n",
        "def filter_video_paths_by_anomaly_type(paths, anomaly_type):\n",
        "    \"\"\"Filter video paths based on a specific anomaly type.\"\"\"\n",
        "    return [path for path in paths if anomaly_type.lower() in path.lower()]\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def can_read_file(filepath):\n",
        "    \"\"\"\n",
        "    This function checks the existence of the file and optionally its extension.\n",
        "\n",
        "    Parameters:\n",
        "    - filepath: Path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the file exists and has a video file extension; False otherwise.\n",
        "    \"\"\"\n",
        "    # List of common video file extensions for basic filtering.\n",
        "    video_extensions = ['.avi', '.mp4', '.mov', '.mkv']\n",
        "\n",
        "    if not os.path.exists(filepath) or not os.path.isfile(filepath):\n",
        "        return False\n",
        "\n",
        "    # Check if the file extension is in the list of video extensions.\n",
        "    file_extension = os.path.splitext(filepath)[1].lower()\n",
        "    if file_extension not in video_extensions:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# Read video paths from the file\n",
        "video_paths = read_video_paths_from_file(train_file_path,base_dataset_dir)\n",
        "\n",
        "# Use \"Shooting\" and \"Normal\" to distinguish the types\n",
        "anomaly_videos = filter_video_paths(video_paths, 'Shooting')\n",
        "normal_videos = filter_video_paths(video_paths, 'Normal')\n",
        "\"\"\"\n",
        "for anomaly_path in anomaly_videos:\n",
        "    print(anomaly_path)\n",
        "\n",
        "for normal_path in normal_videos:\n",
        "    print(normal_path)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KHMuiOcnQZP0",
        "outputId": "998f1525-536b-4363-b4da-79aaaaa6c8cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor anomaly_path in anomaly_videos:\\n    print(anomaly_path)\\n\\nfor normal_path in normal_videos:\\n    print(normal_path)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pm7z6-3UOec"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Resize, normalize and split videos into frames. Batch frames in 32 segments.\n",
        "\n",
        "Use data generators to prevent loading large datset into memory at once, to better utilize memory for resource constrained environement.\n",
        "\n",
        "Generator loads and preprocess 32 frames at each interval."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOu-6P_N6F_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "77GImlkFzaPW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f225e2f9-c2ab-40fd-b999-309ae739581f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nall_videos = anomaly_videos + normal_videos\\nfor video_path in all_videos:\\n    for segment in preprocess_and_segment_video(video_path):\\n        print(f\"Segment shape: {segment.shape} : {video_path}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "def preprocess_and_segment_video(video_input, resize_shape=(224, 224), segment_length=32):\n",
        "    frames = []\n",
        "    is_video_file = isinstance(video_input, str)\n",
        "\n",
        "    if is_video_file:\n",
        "        if not os.path.exists(video_input):\n",
        "            print(f\"File does not exist: {video_input}\")\n",
        "            return\n",
        "        if not can_read_file(video_input):\n",
        "            print(f\"Cannot read file: {video_input}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_input)\n",
        "            if not cap.isOpened():\n",
        "                raise IOError(f\"OpenCV could not open video file for an unknown reason: {video_input}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if frame.size == 0:\n",
        "                print(\"Encountered an empty frame in the video.\")\n",
        "                continue\n",
        "            frame = cv2.resize(frame, resize_shape)\n",
        "            frame = frame.astype(np.float32) / 255.0\n",
        "            frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "    elif isinstance(video_input, (list, np.ndarray)):\n",
        "        # Additional check for non-empty list/array\n",
        "        if len(video_input) == 0:\n",
        "            print(\"The list/array of frames is empty.\")\n",
        "            return\n",
        "\n",
        "        frames = video_input\n",
        "    else:\n",
        "        raise ValueError(\"Invalid type for video_input. Must be a file path or list/array of frames.\")\n",
        "\n",
        "    # Segmenting the frames\n",
        "    for i in range(0, len(frames), segment_length):\n",
        "        segment_frames = frames[i:i+segment_length]\n",
        "        if len(segment_frames) < segment_length:\n",
        "            segment_frames.extend([np.zeros(resize_shape + (3,), dtype=np.float32) for _ in range(segment_length - len(segment_frames))])\n",
        "        yield np.stack(segment_frames, axis=0)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "all_videos = anomaly_videos + normal_videos\n",
        "for video_path in all_videos:\n",
        "    for segment in preprocess_and_segment_video(video_path):\n",
        "        print(f\"Segment shape: {segment.shape} : {video_path}\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIjfSbVTWdsZ"
      },
      "source": [
        "#Video Processing Utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_video_files(directory, extension='mp4'):\n",
        "    \"\"\"\n",
        "    List all video files in the given directory with the specified extension.\n",
        "\n",
        "    Parameters:\n",
        "    - directory: Path to the directory to search for video files.\n",
        "    - extension: Extension of the video files to search for (default is '*.mp4').\n",
        "\n",
        "    Returns:\n",
        "    - A list of paths to the video files found.\n",
        "    \"\"\"\n",
        "     # Check if the directory exists\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Error: Directory does not exist - {directory}\")\n",
        "        return []  # Return an empty list instead of None\n",
        "\n",
        "    # Check if the directory can be accessed (read permissions)\n",
        "    if not os.access(directory, os.R_OK):\n",
        "        print(f\"Error: Directory cannot be read, check permissions - {directory}\")\n",
        "        return []  # Return an empty list instead of None\n",
        "\n",
        "    # Perform a recursive search for video files\n",
        "    pattern = os.path.join(directory, '**', f'*.{extension}')\n",
        "    _video_files = glob.glob(pattern, recursive=True)\n",
        "    return _video_files\n",
        "\n",
        "\n",
        "def shuffle_together(video_paths, labels):\n",
        "    \"\"\"\n",
        "    Shuffle two lists in unison.\n",
        "\n",
        "    Parameters:\n",
        "    - video_paths: The first list to shuffle.\n",
        "    - label: The second list to shuffle, must be the same length as list1.\n",
        "\n",
        "    Returns:\n",
        "    - The shuffled video_paths and label.\n",
        "    \"\"\"\n",
        "    if len(video_paths) != len(labels):\n",
        "        raise ValueError(\"The lists to be shuffled must be the same length.\")\n",
        "\n",
        "    # sklearn's shuffle function to shuffle both lists in unison\n",
        "    video_paths_shuffled, label_shuffled = shuffle(video_paths, labels)\n",
        "    return video_paths_shuffled, label_shuffled\n",
        "\n",
        "def get_total_frames(video_path):\n",
        "    \"\"\"\n",
        "    Returns the total number of frames in a video.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: The path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - The total number of frames as an integer.\n",
        "    \"\"\"\n",
        "    # Initialize the video capture object with the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Failed to open video file: {video_path}\")\n",
        "        return 0  # Indicates that the video could not be opened\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    return total_frames\n",
        "\n",
        "def segments_per_video(video_path, segment_length=32):\n",
        "    total_frames = get_total_frames(video_path)  # This function needs to return the total frame count\n",
        "    segments = 0\n",
        "    if total_frames != 0:\n",
        "          segments = total_frames // segment_length\n",
        "          if total_frames % segment_length != 0:\n",
        "              segments += 1  # Account for the last, potentially shorter, segment\n",
        "    print(\"{} frames, {} segments for {}\".format(total_frames,segments,video_path))\n",
        "    return segments\n",
        "\"\"\"\n",
        "video_path = anomaly_videos[0]\n",
        "total_segments = segments_per_video(video_path, segment_length=32)\n",
        "print(f\"Total segments: {total_segments}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T79crLju9ZIA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1e1026b0-1b3b-44d5-ace6-7c8fcff816f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nvideo_path = anomaly_videos[0]\\ntotal_segments = segments_per_video(video_path, segment_length=32)\\nprint(f\"Total segments: {total_segments}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Features and Labels"
      ],
      "metadata": {
        "id": "oh0-78i09aJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4p6erhh4zIII"
      },
      "outputs": [],
      "source": [
        "def create_dataset_from_videos(video_paths, labels, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Creates a TensorFlow dataset of video segments with corresponding labels.\n",
        "\n",
        "    Args:\n",
        "        video_paths (list of str): Paths to video files.\n",
        "        labels (list of int): Labels for each video file.\n",
        "        resize_shape (tuple): The target shape for resizing frames.\n",
        "        segment_length (int): Number of frames per video segment.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A dataset of video segments and labels.\n",
        "    \"\"\"\n",
        "    def generator():\n",
        "        count = 0;\n",
        "        for video_path, label in zip(video_paths, labels):\n",
        "            count = count + 1;\n",
        "            video_gen = preprocess_and_segment_video(video_path, resize_shape, segment_length)\n",
        "            print(\"Video files count: {} processing: {}\".format(count, video_path))\n",
        "            # Iterate over the generator and yield its items\n",
        "            for segment in video_gen:  # Iterate over items yielded by video_gen\n",
        "                yield segment, label\n",
        "            \"\"\"\n",
        "            if features is not None:\n",
        "                # Yield features and label if processing was successful\n",
        "                yield features, label\n",
        "            else:\n",
        "                print(f\"Skipping video {video_path}, unable to process.\")\n",
        "            \"\"\"\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(segment_length, *resize_shape, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4B-y1mRl1Pm"
      },
      "source": [
        "# Fine-Tune Pretrained Video Swin Transformer Model for Video Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Yzq9StfOfWfv"
      },
      "outputs": [],
      "source": [
        "def build_finetune_model():\n",
        "    \"\"\"\n",
        "    Fine-tune pretrained model.\n",
        "\n",
        "    Parameters:\n",
        "        saved_model_path (str): Path to the saved Video Swin Transformer model.\n",
        "\n",
        "    Returns:\n",
        "        model (keras.Model): Fine-tuned model for shooting classification.\n",
        "    \"\"\"\n",
        "    # Load the pretrained Video Swin model\n",
        "    video_swin = load_model(saved_model_path, compile=False)\n",
        "\n",
        "    # Fine-tuning configuration: set the last N layers to be trainable\n",
        "    # N=1, fine-tune the last layer of the pretrained model\n",
        "    \"\"\"\n",
        "    N=1\n",
        "    for layer in video_swin.layers[:-N]:\n",
        "        layer.trainable = False\n",
        "    for layer in video_swin.layers[-N:]:\n",
        "        layer.trainable = True\n",
        "    \"\"\"\n",
        "    # Set the entire Video Swin model to non-trainable\n",
        "    video_swin.trainable = False\n",
        "\n",
        "    # Downstream model for binary classification\n",
        "    model = Sequential([\n",
        "        video_swin,\n",
        "        # Assume video_swin's output shape is compatible with the Dense layer's input\n",
        "        Dense(512, activation='relu', kernel_regularizer=regularizers.l1(1e-5)),  # Apply L1 regularization\n",
        "        Dropout(0.6),\n",
        "        Dense(32, activation='relu', kernel_regularizer=regularizers.l1(1e-5)),  # Apply L1 regularization again if needed\n",
        "        Dropout(0.6),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7RHFSSomcmd"
      },
      "source": [
        "#Train the Fine-Tuned Model for Shooting Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rL-zPaFnqLq5"
      },
      "outputs": [],
      "source": [
        "# Define the base directory to save checkpoints\n",
        "model_chkpt_filename = f\"training_1/vad_{int(time.time())}.ckpt\"\n",
        "checkpoint_path = os.path.join(base_directory, model_chkpt_filename)\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create the base directory if it doesn't exist\n",
        "os.makedirs(base_directory, exist_ok=True)\n",
        "\n",
        "# Callbacks configuration\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss'),  # Save the best model based on val_loss\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        verbose=1,\n",
        "        patience=10),  # Stop training when `val_loss` is no longer improving\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.1,\n",
        "        patience=5),  # Reduce learning rate when `val_loss` plateaus\n",
        "    TensorBoard(\n",
        "        log_dir=os.path.join(base_directory, 'logs'),  # Path to save log files for TensorBoard in Google Drive\n",
        "        histogram_freq=1,  # Record activation histograms every 1 epoch\n",
        "        embeddings_freq=1)  # Record embedding data every 1 epoch\n",
        "]\n",
        "\n",
        "\n",
        "# Define a custom callback for clearing the session and collecting garbage\n",
        "class ClearSessionCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        tf.keras.backend.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "# Assume `callbacks_list` is already defined. Add the new callback to it.\n",
        "callbacks_list.append(ClearSessionCallback())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Instance Learning Loss"
      ],
      "metadata": {
        "id": "dXmpOUVlqyPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mil_ranking_loss(sparsity_weight=0.01, smoothness_weight=0.01, margin=5.0):\n",
        "    \"\"\"\n",
        "    MIL ranking loss with sparsity and smoothness constraints.\n",
        "\n",
        "    Args:\n",
        "    - sparsity_weight: Weight for the sparsity term.\n",
        "    - smoothness_weight: Weight for the smoothness term.\n",
        "    - margin: Margin for the ranking loss.\n",
        "\n",
        "    Returns:\n",
        "    - A loss function that takes (y_true, y_pred) as inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        # Separate the anomaly scores into positive and negative samples based on the labels.\n",
        "        positive_scores = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        negative_scores = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "\n",
        "        # Calculate the highest scores for positive and negative samples.\n",
        "        highest_positive_score = tf.reduce_max(positive_scores)\n",
        "        highest_negative_score = tf.reduce_max(negative_scores)\n",
        "\n",
        "        # Calculate ranking loss.\n",
        "        ranking_loss = tf.maximum(0.0, margin - highest_positive_score + highest_negative_score)\n",
        "\n",
        "        # Calculate sparsity loss (L1 norm of the predictions).\n",
        "        sparsity_loss = tf.reduce_sum(tf.abs(y_pred))\n",
        "\n",
        "        # Calculate smoothness loss (squared difference between adjacent anomaly scores).\n",
        "        diffs = y_pred[:, 1:] - y_pred[:, :-1]\n",
        "        smoothness_loss = tf.reduce_sum(tf.square(diffs))\n",
        "\n",
        "        # Combine the losses using the weights and loss components.\n",
        "        total_loss = ranking_loss + (sparsity_weight * sparsity_loss) + (smoothness_weight * smoothness_loss)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ra-kgXNIq8Le"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anomaly_videos, normal_videos = list_video_files(anomaly_dir), list_video_files(normal_dir)\n",
        "print(\"\\nanomaly_videos\", len(anomaly_videos))\n",
        "print(\"\\nnormal_videos\", len(normal_videos))\n",
        "\n",
        "video_paths = anomaly_videos + normal_videos\n",
        "labels = [1] * len(anomaly_videos) + [0] * len(normal_videos)\n",
        "print(\"\\nvideo paths {} and labels {}\".format(video_paths,labels))\n",
        "video_paths, labels = shuffle_together(video_paths, labels)\n",
        "print(\"\\nShuffled: video paths {} and labels {}\".format(video_paths,labels))\n",
        "dataset = create_dataset_from_videos(video_paths, labels)\n",
        "\n",
        "\n",
        "cache_data_filename = f\"./cache.tf-data_{int(time.time())}\"\n",
        "dataset = dataset.cache(cache_data_filename)\n",
        "\n",
        "\"\"\"\n",
        "for features, label in dataset.take(3):\n",
        "    print(\"\\nFeatures shape:\", features.numpy().shape)\n",
        "\"\"\"\n",
        "\n",
        "# calculate the total video segments i.e training dataset size\n",
        "total_segments_actual = sum(segments_per_video(video_path, segment_length=32) for video_path in video_paths)\n",
        "print(f\"\\nTotal segments across all videos: {total_segments_actual}\")\n",
        "\n",
        "\"\"\"\n",
        "total_segments_actual = sum(1 for _ in dataset)\n",
        "print(\"\\nTotal segments actually in dataset:\", total_segments_actual)\n",
        "\"\"\"\n",
        "# Calculate split sizes\n",
        "train_data_size = int(total_segments_actual * 0.8)\n",
        "print(\"\\nTraining size split into :\", train_data_size)\n",
        "\n",
        "\n",
        "val_data_size = total_segments_actual - train_data_size\n",
        "\n",
        "print(\"\\nValidation size split into :\", val_data_size)\n",
        "\n",
        "\n",
        "# Ensure dataset is shuffled (use actual total size if known for better shuffling, uses more memory)\n",
        "dataset = dataset.shuffle(buffer_size=(total_segments_actual//2),reshuffle_each_iteration=True)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "train_segments_split = sum(1 for _ in train_dataset)\n",
        "print(\"\\nTraining segments split:\", train_segments_split)\n",
        "\n",
        "val_segments_split = sum(1 for _ in val_dataset)\n",
        "print(\"\\nValidation segments split:\", val_segments_split)\n",
        "\n",
        "for features, label in train_dataset.take(1):\n",
        "    print(\"Train: Features shape:\", features.numpy().shape)\n",
        "    print(\"Train: Label shape:\", label.numpy())\n",
        "    print(\"\\nTrain: Labels:\", label)\n",
        "\n",
        "for features, label in val_dataset.take(1):\n",
        "    print(\"\\nVal: Features shape:\", features.numpy().shape)\n",
        "    print(\"val: Label shape:\", label.numpy())\n",
        "    print(\"val: Labels:\", label)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "xfMMTEpcptbT",
        "outputId": "f9f85df7-4268-4f92-8556-9b1a04885c60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "anomaly_videos 3\n",
            "\n",
            "normal_videos 3\n",
            "\n",
            "video paths ['/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting001_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting012_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting023_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos001_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos002_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos004_x264.mp4'] and labels [1, 1, 1, 0, 0, 0]\n",
            "\n",
            "Shuffled: video paths ['/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting001_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting012_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos004_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting023_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos002_x264.mp4', '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos001_x264.mp4'] and labels [1, 1, 0, 1, 0, 0]\n",
            "253 frames, 8 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting001_x264.mp4\n",
            "1886 frames, 59 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting012_x264.mp4\n",
            "917 frames, 29 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos004_x264.mp4\n",
            "Failed to open video file: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting023_x264.mp4\n",
            "0 frames, 0 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting023_x264.mp4\n",
            "1663 frames, 52 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos002_x264.mp4\n",
            "544 frames, 17 segments for /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos001_x264.mp4\n",
            "\n",
            "Total segments across all videos: 165\n",
            "\n",
            "Training size split into : 132\n",
            "\n",
            "Validation size split into : 33\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_segments_split = sum(1 for _ in train_dataset)\\nprint(\"\\nTraining segments split:\", train_segments_split)\\n\\nval_segments_split = sum(1 for _ in val_dataset)\\nprint(\"\\nValidation segments split:\", val_segments_split)\\n\\nfor features, label in train_dataset.take(1):\\n    print(\"Train: Features shape:\", features.numpy().shape)\\n    print(\"Train: Label shape:\", label.numpy())\\n    print(\"\\nTrain: Labels:\", label)\\n\\nfor features, label in val_dataset.take(1):\\n    print(\"\\nVal: Features shape:\", features.numpy().shape)\\n    print(\"val: Label shape:\", label.numpy())\\n    print(\"val: Labels:\", label)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_correct_dimensions(video_data, labels):\n",
        "    # Now the function directly accepts video_data and labels as separate arguments\n",
        "\n",
        "    # Define the expected shape, excluding batch size to allow dynamic sizes\n",
        "    expected_shape = (32, 224, 224, 3)  # Example: (segments, height, width, channels)\n",
        "\n",
        "    # Ensure the shape of the video data matches the expected shape, excluding the batch size\n",
        "    tf.debugging.assert_equal(tf.shape(video_data)[1:], expected_shape, message=\"Video batch dimensions are incorrect\")\n",
        "\n",
        "    # Return video_data and labels unchanged\n",
        "    return video_data, labels\n"
      ],
      "metadata": {
        "id": "rP3ErVp2ik1q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUKmgvFm43KK",
        "outputId": "27d8d8a0-d3aa-4669-9a33-bbe126f5f44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_steps_per_epoch 16\n",
            "validation_steps_per_epoch 4\n",
            "Checkpoint loaded: /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711698374.ckpt\n",
            "Epoch 1/10\n",
            "Video files count: 1 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting001_x264.mp4\n",
            "Video files count: 2 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting012_x264.mp4\n",
            "Video files count: 3 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos004_x264.mp4\n",
            "Video files count: 4 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting023_x264.mp4\n",
            "Error: OpenCV could not open video file for an unknown reason: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting023_x264.mp4\n",
            "Video files count: 5 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos002_x264.mp4\n",
            "Video files count: 6 processing: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos001_x264.mp4\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 1: val_loss improved from inf to 0.12212, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1162s 71s/step - loss: 0.1282 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1221 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 2: val_loss did not improve from 0.12212\n",
            "16/16 [==============================] - 1077s 68s/step - loss: 0.1306 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1248 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 3: val_loss improved from 0.12212 to 0.12117, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1135s 69s/step - loss: 0.1259 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1212 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 4: val_loss improved from 0.12117 to 0.12115, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1150s 73s/step - loss: 0.1242 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1211 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 5: val_loss improved from 0.12115 to 0.12113, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1129s 71s/step - loss: 0.1230 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1211 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 6: val_loss improved from 0.12113 to 0.12112, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1152s 73s/step - loss: 0.1248 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1211 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 7: val_loss improved from 0.12112 to 0.12110, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1117s 69s/step - loss: 0.1229 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1211 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 8: val_loss improved from 0.12110 to 0.12109, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1202s 76s/step - loss: 0.1223 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1211 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 9: val_loss improved from 0.12109 to 0.12109, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1128s 71s/step - loss: 0.1233 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1211 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 1.0000e-03\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 \n",
            "Epoch 10: val_loss improved from 0.12109 to 0.12108, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n",
            "16/16 [==============================] - 1119s 71s/step - loss: 0.1253 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1211 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 1.0000e-03\n"
          ]
        }
      ],
      "source": [
        "checkpoint_prefix = '/content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711678818'\n",
        "\n",
        "\n",
        "# Batch for memory efficiency\n",
        "batch_size = 8\n",
        "num_epochs = 10\n",
        "\n",
        "train_dataset = dataset.take(train_data_size)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=train_data_size).repeat(num_epochs).batch(batch_size)\n",
        "cache_train_filename = f\"./cache.tf-train_{int(time.time())}\"\n",
        "dataset = dataset.cache(cache_train_filename)\n",
        "\n",
        "val_dataset = dataset.skip(val_data_size)\n",
        "val_dataset = val_dataset.shuffle(buffer_size=val_data_size).repeat(num_epochs).batch(batch_size)\n",
        "\n",
        "\n",
        "val_dataset = train_dataset.cache(filename=\"./cache.tf-val\")\n",
        "cache_val_filename = f\"./cache.tf-val_{int(time.time())}\"\n",
        "dataset = dataset.cache(cache_val_filename)\n",
        "\n",
        "train_dataset = train_dataset.map(ensure_correct_dimensions)\n",
        "val_dataset = val_dataset.map(ensure_correct_dimensions)\n",
        "\n",
        "\"\"\"\n",
        "for features, label in train_dataset.take(1):\n",
        "    print(\"Train: Labels:\", label.numpy())\n",
        "    print(\"Train: Features shape:\", features.numpy().shape)\n",
        "\n",
        "for features, label in val_dataset.take(1):\n",
        "    print(\"Val: Features shape:\", features.numpy().shape)\n",
        "    print(\"val: Labels:\", label.numpy())\n",
        "\"\"\"\n",
        "\n",
        "training_steps_per_epoch = train_data_size // batch_size\n",
        "validation_steps_per_epoch = val_data_size // batch_size\n",
        "\n",
        "print(\"training_steps_per_epoch\",training_steps_per_epoch)\n",
        "print(\"validation_steps_per_epoch\",validation_steps_per_epoch)\n",
        "\n",
        "\n",
        "# Asynchronously fetch batches while model is training\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Configure the Adagrad optimizer\n",
        "adagrad = Adagrad(learning_rate=0.01, epsilon=1e-08)\n",
        "\n",
        "finetune_model = build_finetune_model()\n",
        "\n",
        "# Define the checkpoint directory and prefix\n",
        "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "# Load the latest checkpoint if it exists\n",
        "if latest_checkpoint:\n",
        "    finetune_model.load_weights(latest_checkpoint)\n",
        "    print(f\"Checkpoint loaded: {latest_checkpoint}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "# Compile model with MIL loss, adam optimizer and metrics\n",
        "finetune_model.compile(\n",
        "    optimizer=adagrad,\n",
        "    #loss=mil_ranking_loss(sparsity_weight=0.01, smoothness_weight=0.01),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        Precision(name='precision'),\n",
        "        Recall(name='recall'),\n",
        "        AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "for features, labels in train_dataset.take(1):\n",
        "    print(features.shape)\n",
        "\n",
        "for features, labels in val_dataset.take(1):\n",
        "    print(features.shape)\n",
        "\"\"\"\n",
        "# Train the model\n",
        "history = finetune_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=training_steps_per_epoch,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=validation_steps_per_epoch,  # Add this line\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ced1WK7dnwdb"
      },
      "source": [
        "# Evaluate Final Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7VI6xmkh497Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "59def49e-672f-4e19-9f7f-11c09b100cb0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ae59ffdd9498>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# Plot accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot precision, recall, and AUC\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Precision\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['precision'], label='Train Precision')\n",
        "plt.plot(history.history['val_precision'], label='Validation Precision')\n",
        "plt.title('Precision over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "\n",
        "# Recall\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['recall'], label='Train Recall')\n",
        "plt.plot(history.history['val_recall'], label='Validation Recall')\n",
        "plt.title('Recall over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['auc'], label='Train AUC')\n",
        "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "plt.title('AUC over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "#test_loss, test_accuracy, test_precision, test_recall, test_auc = finetune_model.evaluate(test_dataset)\n",
        "#print(f\"Test Metrics:\\n Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, AUC: {test_auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Fine-Tuned Model\n"
      ],
      "metadata": {
        "id": "5ViGgioyst_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import regularizers\n",
        "import cv2\n",
        "\n",
        "\n",
        "saved_model_path = '/content/drive/MyDrive/VideoAnomalyDetection/pretrained/Video Swin Transformer/TFVideoSwinB_K600_IN22K_P244_W877_32x224'\n",
        "\n",
        "base_dataset_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/'\n",
        "\n",
        "def build_finetune_model():\n",
        "    \"\"\"\n",
        "    Fine-tune pretrained model.\n",
        "\n",
        "    Parameters:\n",
        "        saved_model_path (str): Path to the saved Video Swin Transformer model.\n",
        "\n",
        "    Returns:\n",
        "        model (keras.Model): Fine-tuned model for shooting classification.\n",
        "    \"\"\"\n",
        "    # Load the pretrained Video Swin model\n",
        "    video_swin = load_model(saved_model_path, compile=False)\n",
        "\n",
        "    # Fine-tuning configuration: set the last N layers to be trainable\n",
        "    # N=1, fine-tune the last layer of the pretrained model\n",
        "    \"\"\"\n",
        "    N=1\n",
        "    for layer in video_swin.layers[:-N]:\n",
        "        layer.trainable = False\n",
        "    for layer in video_swin.layers[-N:]:\n",
        "        layer.trainable = True\n",
        "    \"\"\"\n",
        "    # Set the entire Video Swin model to non-trainable\n",
        "    video_swin.trainable = False\n",
        "\n",
        "    # Downstream model for binary classification\n",
        "    model = Sequential([\n",
        "        video_swin,\n",
        "        # Assume video_swin's output shape is compatible with the Dense layer's input\n",
        "        Dense(512, activation='relu', kernel_regularizer=regularizers.l1(1e-5)),  # Apply L1 regularization\n",
        "        Dropout(0.6),\n",
        "        Dense(32, activation='relu', kernel_regularizer=regularizers.l1(1e-5)),  # Apply L1 regularization again if needed\n",
        "        Dropout(0.6),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "def can_read_file(filepath):\n",
        "    \"\"\"\n",
        "    This function checks the existence of the file and optionally its extension.\n",
        "\n",
        "    Parameters:\n",
        "    - filepath: Path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the file exists and has a video file extension; False otherwise.\n",
        "    \"\"\"\n",
        "    # List of common video file extensions for basic filtering.\n",
        "    video_extensions = ['.avi', '.mp4', '.mov', '.mkv']\n",
        "\n",
        "    if not os.path.exists(filepath) or not os.path.isfile(filepath):\n",
        "        return False\n",
        "\n",
        "    # Check if the file extension is in the list of video extensions.\n",
        "    file_extension = os.path.splitext(filepath)[1].lower()\n",
        "    if file_extension not in video_extensions:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def read_video_paths_from_file(file_path, base_dataset_dir):\n",
        "    full_file_path = os.path.join(base_dataset_dir, file_path)\n",
        "\n",
        "    with open(full_file_path, 'r') as file:\n",
        "        # Concatenate base_dataset_dir with each path read from the file\n",
        "        paths = [os.path.join(base_dataset_dir, line.strip()) for line in file]\n",
        "\n",
        "    return paths\n",
        "\n",
        "def filter_video_paths(paths, filter_keyword):\n",
        "    return [path for path in paths if filter_keyword.lower() in path.lower()]\n",
        "\n",
        "\n",
        "def filter_video_paths_by_anomaly_type(paths, anomaly_type):\n",
        "    \"\"\"Filter video paths based on a specific anomaly type.\"\"\"\n",
        "    return [path for path in paths if anomaly_type.lower() in path.lower()]\n",
        "\n",
        "test_file_path = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Temporal_Anomaly_Annotation_for_Testing_Videos_2.txt'\n",
        "\n",
        "\n",
        "# Read video paths from the file\n",
        "video_paths = read_video_paths_from_file(test_file_path,base_dataset_dir)\n",
        "\n",
        "# Use \"Shooting\" and \"Normal\" to distinguish the types\n",
        "anomaly_videos = filter_video_paths(video_paths, 'Shooting')\n",
        "normal_videos = filter_video_paths(video_paths, 'Normal')\n",
        "\n",
        "def preprocess_and_segment_video(video_input, resize_shape=(224, 224), segment_length=32):\n",
        "    frames = []\n",
        "    is_video_file = isinstance(video_input, str)\n",
        "\n",
        "    if is_video_file:\n",
        "        if not os.path.exists(video_input):\n",
        "            print(f\"File does not exist: {video_input}\")\n",
        "            return\n",
        "        if not can_read_file(video_input):\n",
        "            print(f\"Cannot read file: {video_input}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_input)\n",
        "            if not cap.isOpened():\n",
        "                raise IOError(f\"OpenCV could not open video file for an unknown reason: {video_input}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if frame.size == 0:\n",
        "                print(\"Encountered an empty frame in the video.\")\n",
        "                continue\n",
        "            frame = cv2.resize(frame, resize_shape)\n",
        "            frame = frame.astype(np.float32) / 255.0\n",
        "            frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "    elif isinstance(video_input, (list, np.ndarray)):\n",
        "        # Additional check for non-empty list/array\n",
        "        if len(video_input) == 0:\n",
        "            print(\"The list/array of frames is empty.\")\n",
        "            return\n",
        "\n",
        "        frames = video_input\n",
        "    else:\n",
        "        raise ValueError(\"Invalid type for video_input. Must be a file path or list/array of frames.\")\n",
        "\n",
        "    # Segmenting the frames\n",
        "    for i in range(0, len(frames), segment_length):\n",
        "        segment_frames = frames[i:i+segment_length]\n",
        "        if len(segment_frames) < segment_length:\n",
        "            segment_frames.extend([np.zeros(resize_shape + (3,), dtype=np.float32) for _ in range(segment_length - len(segment_frames))])\n",
        "        yield np.stack(segment_frames, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DGyMHad3TT-_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Model Weights for Inference"
      ],
      "metadata": {
        "id": "qsfZx8CM_ELF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/VideoAnomalyDetection/training_1\"\n",
        "test_file_path = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Temporal_Anomaly_Annotation_for_Testing_Videos_2.txt'\n",
        "\n",
        "model = build_finetune_model()\n",
        "\n",
        "# Load the latest checkpoint\n",
        "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "# Check if there is any checkpoint available and load the model or weights\n",
        "if latest_checkpoint:\n",
        "    #If loading the entire model (architecture + weights)\n",
        "    #model = tf.keras.models.load_model(latest_checkpoint)\n",
        "    model.load_weights(latest_checkpoint)\n",
        "    print(f\"Loaded weights from the latest checkpoint: {latest_checkpoint}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Ensure your checkpoint directory is correct.\")"
      ],
      "metadata": {
        "id": "sHPhqV4us1U6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1905d6-f74c-4151-9176-3966b3af802a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._iterations\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded weights from the latest checkpoint: /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1711700518.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Test Videos"
      ],
      "metadata": {
        "id": "RXno3_YQ_TIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model Performance with Test Videos"
      ],
      "metadata": {
        "id": "Vp4lxLNs_aKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_annotations_from_txt(file_path):\n",
        "    \"\"\"\n",
        "    Load annotations from a plain text file and parse them into a structured dictionary.\n",
        "\n",
        "    Expected file format: video_path anomaly_type start1 end1 start2 end2 ...\n",
        "    '-1 -1' indicates the end of annotations for that video.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: Path to the text file containing annotations.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary with video paths as keys and their annotations (types and intervals) as values.\n",
        "    \"\"\"\n",
        "    annotations = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            if not parts:  # Skip empty lines\n",
        "                continue\n",
        "\n",
        "            video_path, anomaly_type = parts[0], parts[1]\n",
        "            intervals = [(int(parts[i]), int(parts[i+1])) for i in range(2, len(parts), 2) if int(parts[i]) >= 0]\n",
        "\n",
        "            annotations[video_path] = {\n",
        "                'type': anomaly_type,\n",
        "                'intervals': intervals\n",
        "            }\n",
        "\n",
        "    return annotations\n",
        "\n",
        "print(load_annotations_from_txt(\"/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Temporal_Anomaly_Annotation_for_Testing_Videos_2.txt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNw_Hn3dr_0B",
        "outputId": "7cbbcb33-06c7-48ad-b646-bff603c2d236"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Small_Anomaly/Shooting007_x264.mp4': {'type': 'Shooting', 'intervals': [(45, 165)]}, 'Small_Normal/Normal_Videos_003_x264.mp4': {'type': 'Normal', 'intervals': []}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_with_segments(model, video_path, resize_shape=(224, 224), segment_length=32):\n",
        "    features = []\n",
        "    for segment in preprocess_and_segment_video(video_path, resize_shape, segment_length):\n",
        "        # Assuming the model can take a 4D input and return a feature vector or classification per segment\n",
        "        segment_features = model.predict(segment[np.newaxis, ...])\n",
        "        features.append(segment_features)\n",
        "    return np.array(features)\n"
      ],
      "metadata": {
        "id": "WthbNgbWnSIJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def validate_binary_targets(y_true, y_pred):\n",
        "    # Convert to integers if they're not already\n",
        "    y_true = [int(y) for y in y_true]\n",
        "    y_pred = [int(y) for y in y_pred]\n",
        "\n",
        "    # Check for equal length\n",
        "    if len(y_true) != len(y_pred):\n",
        "        raise ValueError(\"y_true and y_pred must be of the same length.\")\n",
        "\n",
        "    # Check for binary values only\n",
        "    if set(y_true) > {0, 1} or set(y_pred) > {0, 1}:\n",
        "        raise ValueError(\"Targets must be binary (0 or 1).\")\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "def targeted_evaluate_model(model, video_path, annotations, base_dataset_dir, resize_shape=(224, 224), segment_length=32):\n",
        "    full_video_path = os.path.join(base_dataset_dir, video_path)\n",
        "    predictions = []\n",
        "    truths = []\n",
        "\n",
        "    for anomaly in annotations.get('intervals', []):\n",
        "        # Calculate the segment indices for pre-anomaly, anomaly, and post-anomaly segments\n",
        "        pre_anomaly_start_frame = max(0, anomaly[0] - segment_length)\n",
        "        pre_anomaly_end_frame = anomaly[0]\n",
        "        post_anomaly_start_frame = anomaly[1]\n",
        "        post_anomaly_end_frame = anomaly[1] + segment_length\n",
        "\n",
        "        # Process and predict on the pre-anomaly segment\n",
        "        for segment in preprocess_and_segment_video(full_video_path, start_frame=pre_anomaly_start_frame, end_frame=pre_anomaly_end_frame, resize_shape=resize_shape, segment_length=segment_length):\n",
        "            prediction = model.predict(segment[np.newaxis, ...]) > 0.5\n",
        "            predictions.append(int(prediction))\n",
        "            truths.append(0)  # Assuming 0 represents normal\n",
        "\n",
        "        # Process and predict on the anomalous segment\n",
        "        for segment in preprocess_and_segment_video(full_video_path, start_frame=anomaly[0], end_frame=anomaly[1], resize_shape=resize_shape, segment_length=segment_length):\n",
        "            prediction = model.predict(segment[np.newaxis, ...]) > 0.5\n",
        "            predictions.append(int(prediction))\n",
        "            truths.append(1)  # Assuming 1 represents anomalous\n",
        "\n",
        "        # Process and predict on the post-anomaly segment\n",
        "        for segment in preprocess_and_segment_video(full_video_path, start_frame=post_anomaly_start_frame, end_frame=post_anomaly_end_frame, resize_shape=resize_shape, segment_length=segment_length):\n",
        "            prediction = model.predict(segment[np.newaxis, ...]) > 0.5\n",
        "            predictions.append(int(prediction))\n",
        "            truths.append(0)  # Assuming 0 represents normal\n",
        "\n",
        "    # Thresholding to obtain binary predictions\n",
        "    threshold = 0.5\n",
        "    binary_predictions = [1 if prob > threshold else 0 for prob in predictions]\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(truths, binary_predictions, average='binary')\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def evaluate_model(model, video_path, annotations, base_dataset_dir, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Evaluate the model's performance using segments yielded by preprocess_and_segment_video.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained model to evaluate.\n",
        "    - video_path: Relative path to the video file.\n",
        "    - annotations: Dictionary containing anomaly annotations for the video.\n",
        "    - base_dataset_dir: Base directory where video files are stored.\n",
        "    - resize_shape: The shape to resize each frame of the video.\n",
        "    - segment_length: Number of frames per segment.\n",
        "    \"\"\"\n",
        "    print('video_path',video_path)\n",
        "\n",
        "    path_parts = video_path.split('/')\n",
        "    # Assuming the structure is known and consistent, join the last 3 parts\n",
        "    key = '/'.join(path_parts[-2:])\n",
        "\n",
        "    print('key',key)\n",
        "\n",
        "    annotated_intervals = annotations.get(key, {}).get('intervals', [])\n",
        "    segment_predictions = []\n",
        "    segment_truths = []\n",
        "\n",
        "    print('annotated_intervals 2',annotated_intervals)\n",
        "\n",
        "    # Thresholding to obtain binary predictions\n",
        "    threshold = 0.5\n",
        "    segment_index = 0\n",
        "    for segment in preprocess_and_segment_video(video_path, resize_shape, segment_length):\n",
        "        # Model Prediction\n",
        "        prediction = float(model.predict(segment[np.newaxis, ...])[0])\n",
        "\n",
        "        print(prediction)\n",
        "\n",
        "\n",
        "        # Apply threshold to determine if the segment is predicted as anomalous or normal\n",
        "        predicted_as_anomalous = int(prediction > threshold)  # higher scores indicate anomalies\n",
        "\n",
        "        # Determine if the current segment overlaps with any annotated anomalies\n",
        "        segment_start_frame = segment_index * segment_length\n",
        "        segment_end_frame = segment_start_frame + segment_length\n",
        "\n",
        "        is_anomalous = any(start <= segment_end_frame and end >= segment_start_frame for start, end in annotated_intervals)\n",
        "        print(\"segment_index is {}, segment_start_frame is {}, segment_end_frame is {}, raw prediction is {:.4f}, predicted as anomalous? {}, ground truth: is segment anomalous? {}\".format(\n",
        "        segment_index, segment_start_frame, segment_end_frame, prediction, predicted_as_anomalous, is_anomalous))\n",
        "\n",
        "        # Append the prediction and truth for the current segment\n",
        "        segment_predictions.append(prediction)\n",
        "        segment_truths.append(int(is_anomalous))\n",
        "\n",
        "        segment_index += 1\n",
        "\n",
        "\n",
        "\n",
        "    binary_predictions = [1 if prob > threshold else 0 for prob in segment_predictions]\n",
        "    print('binary_predictions',binary_predictions)\n",
        "    y_true, y_pred = validate_binary_targets(segment_truths, binary_predictions)\n",
        "\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "\n",
        "    print(\"segment_truths length is {}, segment_predictions length is {}\".format(len(segment_truths), len(segment_predictions)))\n",
        "\n",
        "    # Optionally, display segment-level results for analysis\n",
        "    for i, (truth, pred) in enumerate(zip(segment_truths, segment_predictions)):\n",
        "        print(f\"Segment {i+1}: Truth = {truth}, Prediction = {pred}\")\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "# Initialize lists to store the scores for each video\n",
        "all_precisions = []\n",
        "all_recalls = []\n",
        "all_f1_scores = []\n",
        "\n",
        "# Read video paths from the file\n",
        "video_paths = read_video_paths_from_file(test_file_path,base_dataset_dir)\n",
        "\n",
        "# Use \"Shooting\" and \"Normal\" to distinguish the types\n",
        "anomaly_videos = filter_video_paths(video_paths, 'Shooting')\n",
        "normal_videos = filter_video_paths(video_paths, 'Normal')\n",
        "annotation_path = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Temporal_Anomaly_Annotation_for_Testing_Videos_2.txt'\n",
        "annotations = load_annotations_from_txt(annotation_path)\n",
        "# Assuming model is your actual loaded model\n",
        "for video_path in video_paths:\n",
        "    # Extract the actual path from the annotation line, if not already a clean path\n",
        "    clean_video_path = video_path.split()[0]\n",
        "    precision, recall, f1 = evaluate_model(model, clean_video_path, annotations,base_dataset_dir)\n",
        "    # Store the scores\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_f1_scores.append(f1)\n",
        "\n",
        "    # Optionally, print the scores for each video\n",
        "    print(f\"Video: {clean_video_path}\\nPrecision: {precision}, Recall: {recall}, F1 Score: {f1}\\n\")\n",
        "\n",
        "# After evaluating all videos, calculate and print the average scores\n",
        "avg_precision = sum(all_precisions) / len(all_precisions)\n",
        "avg_recall = sum(all_recalls) / len(all_recalls)\n",
        "avg_f1 = sum(all_f1_scores) / len(all_f1_scores)\n",
        "\n",
        "print(f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average F1 Score: {avg_f1}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7POihfKraf3",
        "outputId": "26c2a7f1-c46f-4007-b7b7-caa8e8a282b2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video_path /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting007_x264.mp4\n",
            "key Small_Anomaly/Shooting007_x264.mp4\n",
            "annotated_intervals 2 [(45, 165)]\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.655124843120575\n",
            "segment_index is 0, segment_start_frame is 0, segment_end_frame is 32, raw prediction is 0.6551, predicted as anomalous? 1, ground truth: is segment anomalous? False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-f49de1891532>:92: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  prediction = float(model.predict(segment[np.newaxis, ...])[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n",
            "0.5973549485206604\n",
            "segment_index is 1, segment_start_frame is 32, segment_end_frame is 64, raw prediction is 0.5974, predicted as anomalous? 1, ground truth: is segment anomalous? True\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.5814840793609619\n",
            "segment_index is 2, segment_start_frame is 64, segment_end_frame is 96, raw prediction is 0.5815, predicted as anomalous? 1, ground truth: is segment anomalous? True\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "0.7181258201599121\n",
            "segment_index is 3, segment_start_frame is 96, segment_end_frame is 128, raw prediction is 0.7181, predicted as anomalous? 1, ground truth: is segment anomalous? True\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.3839857578277588\n",
            "segment_index is 4, segment_start_frame is 128, segment_end_frame is 160, raw prediction is 0.3840, predicted as anomalous? 0, ground truth: is segment anomalous? True\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.45263397693634033\n",
            "segment_index is 5, segment_start_frame is 160, segment_end_frame is 192, raw prediction is 0.4526, predicted as anomalous? 0, ground truth: is segment anomalous? True\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.4574790596961975\n",
            "segment_index is 6, segment_start_frame is 192, segment_end_frame is 224, raw prediction is 0.4575, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.5117754936218262\n",
            "segment_index is 7, segment_start_frame is 224, segment_end_frame is 256, raw prediction is 0.5118, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9972220063209534\n",
            "segment_index is 8, segment_start_frame is 256, segment_end_frame is 288, raw prediction is 0.9972, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998507499694824\n",
            "segment_index is 9, segment_start_frame is 288, segment_end_frame is 320, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.6788578033447266\n",
            "segment_index is 10, segment_start_frame is 320, segment_end_frame is 352, raw prediction is 0.6789, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.40966594219207764\n",
            "segment_index is 11, segment_start_frame is 352, segment_end_frame is 384, raw prediction is 0.4097, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.7133090496063232\n",
            "segment_index is 12, segment_start_frame is 384, segment_end_frame is 416, raw prediction is 0.7133, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "0.9988545179367065\n",
            "segment_index is 13, segment_start_frame is 416, segment_end_frame is 448, raw prediction is 0.9989, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9996790885925293\n",
            "segment_index is 14, segment_start_frame is 448, segment_end_frame is 480, raw prediction is 0.9997, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999982118606567\n",
            "segment_index is 15, segment_start_frame is 480, segment_end_frame is 512, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999872446060181\n",
            "segment_index is 16, segment_start_frame is 512, segment_end_frame is 544, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999863505363464\n",
            "segment_index is 17, segment_start_frame is 544, segment_end_frame is 576, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999483227729797\n",
            "segment_index is 18, segment_start_frame is 576, segment_end_frame is 608, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999462962150574\n",
            "segment_index is 19, segment_start_frame is 608, segment_end_frame is 640, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999451041221619\n",
            "segment_index is 20, segment_start_frame is 640, segment_end_frame is 672, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9988791346549988\n",
            "segment_index is 21, segment_start_frame is 672, segment_end_frame is 704, raw prediction is 0.9989, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9990384578704834\n",
            "segment_index is 22, segment_start_frame is 704, segment_end_frame is 736, raw prediction is 0.9990, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9992430210113525\n",
            "segment_index is 23, segment_start_frame is 736, segment_end_frame is 768, raw prediction is 0.9992, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9994909763336182\n",
            "segment_index is 24, segment_start_frame is 768, segment_end_frame is 800, raw prediction is 0.9995, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.8558244109153748\n",
            "segment_index is 25, segment_start_frame is 800, segment_end_frame is 832, raw prediction is 0.8558, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.5754562020301819\n",
            "segment_index is 26, segment_start_frame is 832, segment_end_frame is 864, raw prediction is 0.5755, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.49336618185043335\n",
            "segment_index is 27, segment_start_frame is 864, segment_end_frame is 896, raw prediction is 0.4934, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "0.5102317333221436\n",
            "segment_index is 28, segment_start_frame is 896, segment_end_frame is 928, raw prediction is 0.5102, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.6198801398277283\n",
            "segment_index is 29, segment_start_frame is 928, segment_end_frame is 960, raw prediction is 0.6199, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.7341659069061279\n",
            "segment_index is 30, segment_start_frame is 960, segment_end_frame is 992, raw prediction is 0.7342, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9079186916351318\n",
            "segment_index is 31, segment_start_frame is 992, segment_end_frame is 1024, raw prediction is 0.9079, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.6838622093200684\n",
            "segment_index is 32, segment_start_frame is 1024, segment_end_frame is 1056, raw prediction is 0.6839, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.6696444749832153\n",
            "segment_index is 33, segment_start_frame is 1056, segment_end_frame is 1088, raw prediction is 0.6696, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.4717119038105011\n",
            "segment_index is 34, segment_start_frame is 1088, segment_end_frame is 1120, raw prediction is 0.4717, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.38200831413269043\n",
            "segment_index is 35, segment_start_frame is 1120, segment_end_frame is 1152, raw prediction is 0.3820, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.3890969455242157\n",
            "segment_index is 36, segment_start_frame is 1152, segment_end_frame is 1184, raw prediction is 0.3891, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.41521307826042175\n",
            "segment_index is 37, segment_start_frame is 1184, segment_end_frame is 1216, raw prediction is 0.4152, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.48984599113464355\n",
            "segment_index is 38, segment_start_frame is 1216, segment_end_frame is 1248, raw prediction is 0.4898, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.48469141125679016\n",
            "segment_index is 39, segment_start_frame is 1248, segment_end_frame is 1280, raw prediction is 0.4847, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.48790913820266724\n",
            "segment_index is 40, segment_start_frame is 1280, segment_end_frame is 1312, raw prediction is 0.4879, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.49094662070274353\n",
            "segment_index is 41, segment_start_frame is 1312, segment_end_frame is 1344, raw prediction is 0.4909, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.5187992453575134\n",
            "segment_index is 42, segment_start_frame is 1344, segment_end_frame is 1376, raw prediction is 0.5188, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.49821779131889343\n",
            "segment_index is 43, segment_start_frame is 1376, segment_end_frame is 1408, raw prediction is 0.4982, predicted as anomalous? 0, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9887480735778809\n",
            "segment_index is 44, segment_start_frame is 1408, segment_end_frame is 1440, raw prediction is 0.9887, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "binary_predictions [1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
            "segment_truths length is 45, segment_predictions length is 45\n",
            "Segment 1: Truth = 0, Prediction = 0.655124843120575\n",
            "Segment 2: Truth = 1, Prediction = 0.5973549485206604\n",
            "Segment 3: Truth = 1, Prediction = 0.5814840793609619\n",
            "Segment 4: Truth = 1, Prediction = 0.7181258201599121\n",
            "Segment 5: Truth = 1, Prediction = 0.3839857578277588\n",
            "Segment 6: Truth = 1, Prediction = 0.45263397693634033\n",
            "Segment 7: Truth = 0, Prediction = 0.4574790596961975\n",
            "Segment 8: Truth = 0, Prediction = 0.5117754936218262\n",
            "Segment 9: Truth = 0, Prediction = 0.9972220063209534\n",
            "Segment 10: Truth = 0, Prediction = 0.9998507499694824\n",
            "Segment 11: Truth = 0, Prediction = 0.6788578033447266\n",
            "Segment 12: Truth = 0, Prediction = 0.40966594219207764\n",
            "Segment 13: Truth = 0, Prediction = 0.7133090496063232\n",
            "Segment 14: Truth = 0, Prediction = 0.9988545179367065\n",
            "Segment 15: Truth = 0, Prediction = 0.9996790885925293\n",
            "Segment 16: Truth = 0, Prediction = 0.9999982118606567\n",
            "Segment 17: Truth = 0, Prediction = 0.9999872446060181\n",
            "Segment 18: Truth = 0, Prediction = 0.9999863505363464\n",
            "Segment 19: Truth = 0, Prediction = 0.9999483227729797\n",
            "Segment 20: Truth = 0, Prediction = 0.9999462962150574\n",
            "Segment 21: Truth = 0, Prediction = 0.9999451041221619\n",
            "Segment 22: Truth = 0, Prediction = 0.9988791346549988\n",
            "Segment 23: Truth = 0, Prediction = 0.9990384578704834\n",
            "Segment 24: Truth = 0, Prediction = 0.9992430210113525\n",
            "Segment 25: Truth = 0, Prediction = 0.9994909763336182\n",
            "Segment 26: Truth = 0, Prediction = 0.8558244109153748\n",
            "Segment 27: Truth = 0, Prediction = 0.5754562020301819\n",
            "Segment 28: Truth = 0, Prediction = 0.49336618185043335\n",
            "Segment 29: Truth = 0, Prediction = 0.5102317333221436\n",
            "Segment 30: Truth = 0, Prediction = 0.6198801398277283\n",
            "Segment 31: Truth = 0, Prediction = 0.7341659069061279\n",
            "Segment 32: Truth = 0, Prediction = 0.9079186916351318\n",
            "Segment 33: Truth = 0, Prediction = 0.6838622093200684\n",
            "Segment 34: Truth = 0, Prediction = 0.6696444749832153\n",
            "Segment 35: Truth = 0, Prediction = 0.4717119038105011\n",
            "Segment 36: Truth = 0, Prediction = 0.38200831413269043\n",
            "Segment 37: Truth = 0, Prediction = 0.3890969455242157\n",
            "Segment 38: Truth = 0, Prediction = 0.41521307826042175\n",
            "Segment 39: Truth = 0, Prediction = 0.48984599113464355\n",
            "Segment 40: Truth = 0, Prediction = 0.48469141125679016\n",
            "Segment 41: Truth = 0, Prediction = 0.48790913820266724\n",
            "Segment 42: Truth = 0, Prediction = 0.49094662070274353\n",
            "Segment 43: Truth = 0, Prediction = 0.5187992453575134\n",
            "Segment 44: Truth = 0, Prediction = 0.49821779131889343\n",
            "Segment 45: Truth = 0, Prediction = 0.9887480735778809\n",
            "Video: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Anomaly/Shooting007_x264.mp4\n",
            "Precision: 0.0967741935483871, Recall: 0.6, F1 Score: 0.16666666666666666\n",
            "\n",
            "video_path /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos_003_x264.mp4\n",
            "key Small_Normal/Normal_Videos_003_x264.mp4\n",
            "annotated_intervals 2 []\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9994868636131287\n",
            "segment_index is 0, segment_start_frame is 0, segment_end_frame is 32, raw prediction is 0.9995, predicted as anomalous? 1, ground truth: is segment anomalous? False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-f49de1891532>:92: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  prediction = float(model.predict(segment[np.newaxis, ...])[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9986885786056519\n",
            "segment_index is 1, segment_start_frame is 32, segment_end_frame is 64, raw prediction is 0.9987, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.996228814125061\n",
            "segment_index is 2, segment_start_frame is 64, segment_end_frame is 96, raw prediction is 0.9962, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9920499324798584\n",
            "segment_index is 3, segment_start_frame is 96, segment_end_frame is 128, raw prediction is 0.9920, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9975340962409973\n",
            "segment_index is 4, segment_start_frame is 128, segment_end_frame is 160, raw prediction is 0.9975, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.989021897315979\n",
            "segment_index is 5, segment_start_frame is 160, segment_end_frame is 192, raw prediction is 0.9890, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.991450309753418\n",
            "segment_index is 6, segment_start_frame is 192, segment_end_frame is 224, raw prediction is 0.9915, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9905790090560913\n",
            "segment_index is 7, segment_start_frame is 224, segment_end_frame is 256, raw prediction is 0.9906, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9980414509773254\n",
            "segment_index is 8, segment_start_frame is 256, segment_end_frame is 288, raw prediction is 0.9980, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9958959221839905\n",
            "segment_index is 9, segment_start_frame is 288, segment_end_frame is 320, raw prediction is 0.9959, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9983917474746704\n",
            "segment_index is 10, segment_start_frame is 320, segment_end_frame is 352, raw prediction is 0.9984, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9983198046684265\n",
            "segment_index is 11, segment_start_frame is 352, segment_end_frame is 384, raw prediction is 0.9983, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.999474823474884\n",
            "segment_index is 12, segment_start_frame is 384, segment_end_frame is 416, raw prediction is 0.9995, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9958357810974121\n",
            "segment_index is 13, segment_start_frame is 416, segment_end_frame is 448, raw prediction is 0.9958, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9925456643104553\n",
            "segment_index is 14, segment_start_frame is 448, segment_end_frame is 480, raw prediction is 0.9925, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9934971928596497\n",
            "segment_index is 15, segment_start_frame is 480, segment_end_frame is 512, raw prediction is 0.9935, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9994816184043884\n",
            "segment_index is 16, segment_start_frame is 512, segment_end_frame is 544, raw prediction is 0.9995, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9901244044303894\n",
            "segment_index is 17, segment_start_frame is 544, segment_end_frame is 576, raw prediction is 0.9901, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9945616126060486\n",
            "segment_index is 18, segment_start_frame is 576, segment_end_frame is 608, raw prediction is 0.9946, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9986482858657837\n",
            "segment_index is 19, segment_start_frame is 608, segment_end_frame is 640, raw prediction is 0.9986, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998888373374939\n",
            "segment_index is 20, segment_start_frame is 640, segment_end_frame is 672, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9992738962173462\n",
            "segment_index is 21, segment_start_frame is 672, segment_end_frame is 704, raw prediction is 0.9993, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9971740245819092\n",
            "segment_index is 22, segment_start_frame is 704, segment_end_frame is 736, raw prediction is 0.9972, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9928438067436218\n",
            "segment_index is 23, segment_start_frame is 736, segment_end_frame is 768, raw prediction is 0.9928, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9989309310913086\n",
            "segment_index is 24, segment_start_frame is 768, segment_end_frame is 800, raw prediction is 0.9989, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.998320996761322\n",
            "segment_index is 25, segment_start_frame is 800, segment_end_frame is 832, raw prediction is 0.9983, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9996703267097473\n",
            "segment_index is 26, segment_start_frame is 832, segment_end_frame is 864, raw prediction is 0.9997, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9984384179115295\n",
            "segment_index is 27, segment_start_frame is 864, segment_end_frame is 896, raw prediction is 0.9984, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.987760066986084\n",
            "segment_index is 28, segment_start_frame is 896, segment_end_frame is 928, raw prediction is 0.9878, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9991298317909241\n",
            "segment_index is 29, segment_start_frame is 928, segment_end_frame is 960, raw prediction is 0.9991, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999803900718689\n",
            "segment_index is 30, segment_start_frame is 960, segment_end_frame is 992, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998897910118103\n",
            "segment_index is 31, segment_start_frame is 992, segment_end_frame is 1024, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "0.9998800754547119\n",
            "segment_index is 32, segment_start_frame is 1024, segment_end_frame is 1056, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999033808708191\n",
            "segment_index is 33, segment_start_frame is 1056, segment_end_frame is 1088, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999771118164062\n",
            "segment_index is 34, segment_start_frame is 1088, segment_end_frame is 1120, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999968409538269\n",
            "segment_index is 35, segment_start_frame is 1120, segment_end_frame is 1152, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999307990074158\n",
            "segment_index is 36, segment_start_frame is 1152, segment_end_frame is 1184, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9861941933631897\n",
            "segment_index is 37, segment_start_frame is 1184, segment_end_frame is 1216, raw prediction is 0.9862, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9951213002204895\n",
            "segment_index is 38, segment_start_frame is 1216, segment_end_frame is 1248, raw prediction is 0.9951, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9997036457061768\n",
            "segment_index is 39, segment_start_frame is 1248, segment_end_frame is 1280, raw prediction is 0.9997, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999927878379822\n",
            "segment_index is 40, segment_start_frame is 1280, segment_end_frame is 1312, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9975880980491638\n",
            "segment_index is 41, segment_start_frame is 1312, segment_end_frame is 1344, raw prediction is 0.9976, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999232292175293\n",
            "segment_index is 42, segment_start_frame is 1344, segment_end_frame is 1376, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9929929375648499\n",
            "segment_index is 43, segment_start_frame is 1376, segment_end_frame is 1408, raw prediction is 0.9930, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9973174929618835\n",
            "segment_index is 44, segment_start_frame is 1408, segment_end_frame is 1440, raw prediction is 0.9973, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999114871025085\n",
            "segment_index is 45, segment_start_frame is 1440, segment_end_frame is 1472, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998176097869873\n",
            "segment_index is 46, segment_start_frame is 1472, segment_end_frame is 1504, raw prediction is 0.9998, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.99984210729599\n",
            "segment_index is 47, segment_start_frame is 1504, segment_end_frame is 1536, raw prediction is 0.9998, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998540878295898\n",
            "segment_index is 48, segment_start_frame is 1536, segment_end_frame is 1568, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998205304145813\n",
            "segment_index is 49, segment_start_frame is 1568, segment_end_frame is 1600, raw prediction is 0.9998, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998385310173035\n",
            "segment_index is 50, segment_start_frame is 1600, segment_end_frame is 1632, raw prediction is 0.9998, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998024702072144\n",
            "segment_index is 51, segment_start_frame is 1632, segment_end_frame is 1664, raw prediction is 0.9998, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9994969964027405\n",
            "segment_index is 52, segment_start_frame is 1664, segment_end_frame is 1696, raw prediction is 0.9995, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999561905860901\n",
            "segment_index is 53, segment_start_frame is 1696, segment_end_frame is 1728, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9995723962783813\n",
            "segment_index is 54, segment_start_frame is 1728, segment_end_frame is 1760, raw prediction is 0.9996, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9997507333755493\n",
            "segment_index is 55, segment_start_frame is 1760, segment_end_frame is 1792, raw prediction is 0.9998, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9982301592826843\n",
            "segment_index is 56, segment_start_frame is 1792, segment_end_frame is 1824, raw prediction is 0.9982, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9990213513374329\n",
            "segment_index is 57, segment_start_frame is 1824, segment_end_frame is 1856, raw prediction is 0.9990, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9999664425849915\n",
            "segment_index is 58, segment_start_frame is 1856, segment_end_frame is 1888, raw prediction is 1.0000, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9997809529304504\n",
            "segment_index is 59, segment_start_frame is 1888, segment_end_frame is 1920, raw prediction is 0.9998, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9988834857940674\n",
            "segment_index is 60, segment_start_frame is 1920, segment_end_frame is 1952, raw prediction is 0.9989, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9986444115638733\n",
            "segment_index is 61, segment_start_frame is 1952, segment_end_frame is 1984, raw prediction is 0.9986, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9962469935417175\n",
            "segment_index is 62, segment_start_frame is 1984, segment_end_frame is 2016, raw prediction is 0.9962, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9952043294906616\n",
            "segment_index is 63, segment_start_frame is 2016, segment_end_frame is 2048, raw prediction is 0.9952, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9962171316146851\n",
            "segment_index is 64, segment_start_frame is 2048, segment_end_frame is 2080, raw prediction is 0.9962, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9992175102233887\n",
            "segment_index is 65, segment_start_frame is 2080, segment_end_frame is 2112, raw prediction is 0.9992, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9996802806854248\n",
            "segment_index is 66, segment_start_frame is 2112, segment_end_frame is 2144, raw prediction is 0.9997, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9917002320289612\n",
            "segment_index is 67, segment_start_frame is 2144, segment_end_frame is 2176, raw prediction is 0.9917, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9948599934577942\n",
            "segment_index is 68, segment_start_frame is 2176, segment_end_frame is 2208, raw prediction is 0.9949, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9967602491378784\n",
            "segment_index is 69, segment_start_frame is 2208, segment_end_frame is 2240, raw prediction is 0.9968, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9967504739761353\n",
            "segment_index is 70, segment_start_frame is 2240, segment_end_frame is 2272, raw prediction is 0.9968, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9935173988342285\n",
            "segment_index is 71, segment_start_frame is 2272, segment_end_frame is 2304, raw prediction is 0.9935, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9913560748100281\n",
            "segment_index is 72, segment_start_frame is 2304, segment_end_frame is 2336, raw prediction is 0.9914, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9813591837882996\n",
            "segment_index is 73, segment_start_frame is 2336, segment_end_frame is 2368, raw prediction is 0.9814, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9817968606948853\n",
            "segment_index is 74, segment_start_frame is 2368, segment_end_frame is 2400, raw prediction is 0.9818, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9947589635848999\n",
            "segment_index is 75, segment_start_frame is 2400, segment_end_frame is 2432, raw prediction is 0.9948, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9963765144348145\n",
            "segment_index is 76, segment_start_frame is 2432, segment_end_frame is 2464, raw prediction is 0.9964, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9985785484313965\n",
            "segment_index is 77, segment_start_frame is 2464, segment_end_frame is 2496, raw prediction is 0.9986, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9991871118545532\n",
            "segment_index is 78, segment_start_frame is 2496, segment_end_frame is 2528, raw prediction is 0.9992, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9989656209945679\n",
            "segment_index is 79, segment_start_frame is 2528, segment_end_frame is 2560, raw prediction is 0.9990, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9997507929801941\n",
            "segment_index is 80, segment_start_frame is 2560, segment_end_frame is 2592, raw prediction is 0.9998, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9971965551376343\n",
            "segment_index is 81, segment_start_frame is 2592, segment_end_frame is 2624, raw prediction is 0.9972, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9972727298736572\n",
            "segment_index is 82, segment_start_frame is 2624, segment_end_frame is 2656, raw prediction is 0.9973, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.995509684085846\n",
            "segment_index is 83, segment_start_frame is 2656, segment_end_frame is 2688, raw prediction is 0.9955, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9981051087379456\n",
            "segment_index is 84, segment_start_frame is 2688, segment_end_frame is 2720, raw prediction is 0.9981, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9998568892478943\n",
            "segment_index is 85, segment_start_frame is 2720, segment_end_frame is 2752, raw prediction is 0.9999, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9996740221977234\n",
            "segment_index is 86, segment_start_frame is 2752, segment_end_frame is 2784, raw prediction is 0.9997, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9994408488273621\n",
            "segment_index is 87, segment_start_frame is 2784, segment_end_frame is 2816, raw prediction is 0.9994, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0.9964897036552429\n",
            "segment_index is 88, segment_start_frame is 2816, segment_end_frame is 2848, raw prediction is 0.9965, predicted as anomalous? 1, ground truth: is segment anomalous? False\n",
            "binary_predictions [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "segment_truths length is 89, segment_predictions length is 89\n",
            "Segment 1: Truth = 0, Prediction = 0.9994868636131287\n",
            "Segment 2: Truth = 0, Prediction = 0.9986885786056519\n",
            "Segment 3: Truth = 0, Prediction = 0.996228814125061\n",
            "Segment 4: Truth = 0, Prediction = 0.9920499324798584\n",
            "Segment 5: Truth = 0, Prediction = 0.9975340962409973\n",
            "Segment 6: Truth = 0, Prediction = 0.989021897315979\n",
            "Segment 7: Truth = 0, Prediction = 0.991450309753418\n",
            "Segment 8: Truth = 0, Prediction = 0.9905790090560913\n",
            "Segment 9: Truth = 0, Prediction = 0.9980414509773254\n",
            "Segment 10: Truth = 0, Prediction = 0.9958959221839905\n",
            "Segment 11: Truth = 0, Prediction = 0.9983917474746704\n",
            "Segment 12: Truth = 0, Prediction = 0.9983198046684265\n",
            "Segment 13: Truth = 0, Prediction = 0.999474823474884\n",
            "Segment 14: Truth = 0, Prediction = 0.9958357810974121\n",
            "Segment 15: Truth = 0, Prediction = 0.9925456643104553\n",
            "Segment 16: Truth = 0, Prediction = 0.9934971928596497\n",
            "Segment 17: Truth = 0, Prediction = 0.9994816184043884\n",
            "Segment 18: Truth = 0, Prediction = 0.9901244044303894\n",
            "Segment 19: Truth = 0, Prediction = 0.9945616126060486\n",
            "Segment 20: Truth = 0, Prediction = 0.9986482858657837\n",
            "Segment 21: Truth = 0, Prediction = 0.9998888373374939\n",
            "Segment 22: Truth = 0, Prediction = 0.9992738962173462\n",
            "Segment 23: Truth = 0, Prediction = 0.9971740245819092\n",
            "Segment 24: Truth = 0, Prediction = 0.9928438067436218\n",
            "Segment 25: Truth = 0, Prediction = 0.9989309310913086\n",
            "Segment 26: Truth = 0, Prediction = 0.998320996761322\n",
            "Segment 27: Truth = 0, Prediction = 0.9996703267097473\n",
            "Segment 28: Truth = 0, Prediction = 0.9984384179115295\n",
            "Segment 29: Truth = 0, Prediction = 0.987760066986084\n",
            "Segment 30: Truth = 0, Prediction = 0.9991298317909241\n",
            "Segment 31: Truth = 0, Prediction = 0.9999803900718689\n",
            "Segment 32: Truth = 0, Prediction = 0.9998897910118103\n",
            "Segment 33: Truth = 0, Prediction = 0.9998800754547119\n",
            "Segment 34: Truth = 0, Prediction = 0.9999033808708191\n",
            "Segment 35: Truth = 0, Prediction = 0.9999771118164062\n",
            "Segment 36: Truth = 0, Prediction = 0.9999968409538269\n",
            "Segment 37: Truth = 0, Prediction = 0.9999307990074158\n",
            "Segment 38: Truth = 0, Prediction = 0.9861941933631897\n",
            "Segment 39: Truth = 0, Prediction = 0.9951213002204895\n",
            "Segment 40: Truth = 0, Prediction = 0.9997036457061768\n",
            "Segment 41: Truth = 0, Prediction = 0.9999927878379822\n",
            "Segment 42: Truth = 0, Prediction = 0.9975880980491638\n",
            "Segment 43: Truth = 0, Prediction = 0.9999232292175293\n",
            "Segment 44: Truth = 0, Prediction = 0.9929929375648499\n",
            "Segment 45: Truth = 0, Prediction = 0.9973174929618835\n",
            "Segment 46: Truth = 0, Prediction = 0.9999114871025085\n",
            "Segment 47: Truth = 0, Prediction = 0.9998176097869873\n",
            "Segment 48: Truth = 0, Prediction = 0.99984210729599\n",
            "Segment 49: Truth = 0, Prediction = 0.9998540878295898\n",
            "Segment 50: Truth = 0, Prediction = 0.9998205304145813\n",
            "Segment 51: Truth = 0, Prediction = 0.9998385310173035\n",
            "Segment 52: Truth = 0, Prediction = 0.9998024702072144\n",
            "Segment 53: Truth = 0, Prediction = 0.9994969964027405\n",
            "Segment 54: Truth = 0, Prediction = 0.9999561905860901\n",
            "Segment 55: Truth = 0, Prediction = 0.9995723962783813\n",
            "Segment 56: Truth = 0, Prediction = 0.9997507333755493\n",
            "Segment 57: Truth = 0, Prediction = 0.9982301592826843\n",
            "Segment 58: Truth = 0, Prediction = 0.9990213513374329\n",
            "Segment 59: Truth = 0, Prediction = 0.9999664425849915\n",
            "Segment 60: Truth = 0, Prediction = 0.9997809529304504\n",
            "Segment 61: Truth = 0, Prediction = 0.9988834857940674\n",
            "Segment 62: Truth = 0, Prediction = 0.9986444115638733\n",
            "Segment 63: Truth = 0, Prediction = 0.9962469935417175\n",
            "Segment 64: Truth = 0, Prediction = 0.9952043294906616\n",
            "Segment 65: Truth = 0, Prediction = 0.9962171316146851\n",
            "Segment 66: Truth = 0, Prediction = 0.9992175102233887\n",
            "Segment 67: Truth = 0, Prediction = 0.9996802806854248\n",
            "Segment 68: Truth = 0, Prediction = 0.9917002320289612\n",
            "Segment 69: Truth = 0, Prediction = 0.9948599934577942\n",
            "Segment 70: Truth = 0, Prediction = 0.9967602491378784\n",
            "Segment 71: Truth = 0, Prediction = 0.9967504739761353\n",
            "Segment 72: Truth = 0, Prediction = 0.9935173988342285\n",
            "Segment 73: Truth = 0, Prediction = 0.9913560748100281\n",
            "Segment 74: Truth = 0, Prediction = 0.9813591837882996\n",
            "Segment 75: Truth = 0, Prediction = 0.9817968606948853\n",
            "Segment 76: Truth = 0, Prediction = 0.9947589635848999\n",
            "Segment 77: Truth = 0, Prediction = 0.9963765144348145\n",
            "Segment 78: Truth = 0, Prediction = 0.9985785484313965\n",
            "Segment 79: Truth = 0, Prediction = 0.9991871118545532\n",
            "Segment 80: Truth = 0, Prediction = 0.9989656209945679\n",
            "Segment 81: Truth = 0, Prediction = 0.9997507929801941\n",
            "Segment 82: Truth = 0, Prediction = 0.9971965551376343\n",
            "Segment 83: Truth = 0, Prediction = 0.9972727298736572\n",
            "Segment 84: Truth = 0, Prediction = 0.995509684085846\n",
            "Segment 85: Truth = 0, Prediction = 0.9981051087379456\n",
            "Segment 86: Truth = 0, Prediction = 0.9998568892478943\n",
            "Segment 87: Truth = 0, Prediction = 0.9996740221977234\n",
            "Segment 88: Truth = 0, Prediction = 0.9994408488273621\n",
            "Segment 89: Truth = 0, Prediction = 0.9964897036552429\n",
            "Video: /content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Small_Normal/Normal_Videos_003_x264.mp4\n",
            "Precision: 0.0, Recall: 0.0, F1 Score: 0.0\n",
            "\n",
            "Average Precision: 0.04838709677419355, Average Recall: 0.3, Average F1 Score: 0.08333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Categories for the x-axis\n",
        "categories = ['Precision', 'Recall', 'F1 Score']\n",
        "\n",
        "# Average scores for the y-axis\n",
        "average_scores = [avg_precision, avg_recall, avg_f1]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(categories, average_scores, color=['blue', 'green', 'red'])\n",
        "\n",
        "plt.title('Average Model Performance Scores')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)  # Assuming scores are between 0 and 1\n",
        "\n",
        "# Display the actual score above each bar for clarity\n",
        "for i, score in enumerate(average_scores):\n",
        "    plt.text(i, score + 0.02, f\"{score:.2f}\", ha='center')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-4tyKMoGiR1a",
        "outputId": "8ea2cb6d-248b-4cb2-a58d-7c6f2b97528c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0aMTASstsD7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/DotunOluyade/ShootingVideoClassifier/blob/main/VideoAnomalyDetection.ipynb",
      "authorship_tag": "ABX9TyNPI7aILe1TtMeWIBLkPCtm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}