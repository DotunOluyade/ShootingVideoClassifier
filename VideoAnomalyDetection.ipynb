{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DotunOluyade/ShootingVideoClassifier/blob/main/VideoAnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqG2VmTbSxG7"
      },
      "source": [
        "\n",
        "# Real-Time Crime Prevention using Video Anomaly Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparative Analysis of Pretrained Video Classification models in detecting real-time shooting crime for peace and safety"
      ],
      "metadata": {
        "id": "014sqOa1E1R_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAZ2GqfATJYM"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "A sub-set of UCF-Crime dataset is used for this research work. It contains 128 hours of video, comprising 1900 long untrimmed real world surveillance videos, with 13 realistic anomalies as well as normal activities (Sultani et al., *2018*).\n",
        "\n",
        "This research uses the shooting dataset only to fine-tune selected video classification pretrained models, compares and evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify Dataset Location\n",
        "\n",
        "Import package dependencies and define variables for datasets and pretrained model.\n",
        "\n",
        "Video Swim a pure transformer based video modeling algorithm with its pretrained model is used for feature extraction and fine-tuned with the shooting dataset for classifying videos as shooting or non shooting videos (Liu et al.,2022).\n",
        "\n"
      ],
      "metadata": {
        "id": "uparbOrXN11d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle as sk_shuffle\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "normal_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Training_Normal_Videos_Anomaly'\n",
        "anomaly_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Shooting'\n",
        "\n",
        "saved_model_path = '/content/drive/MyDrive/VideoAnomalyDetection/pretrained/Video Swin Transformer/TFVideoSwinB_K600_IN22K_P244_W877_32x224'"
      ],
      "metadata": {
        "id": "seFZ6F6PTF71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function for Directory Listing of Video Files"
      ],
      "metadata": {
        "id": "GhAk8_f67AxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_video_files(directory, extension='*.mp4'):\n",
        "    \"\"\"\n",
        "    List all video files in the given directory with the specified extension.\n",
        "\n",
        "    Parameters:\n",
        "    - directory: Path to the directory to search for video files.\n",
        "    - extension: Extension of the video files to search for (default is '*.mp4').\n",
        "\n",
        "    Returns:\n",
        "    - A list of paths to the video files found.\n",
        "    \"\"\"\n",
        "    # Create a full path pattern by joining directory and extension\n",
        "    pattern = os.path.join(directory, extension)\n",
        "    # Use glob to find all files matching the pattern\n",
        "    video_files = glob.glob(pattern)\n",
        "    return video_files"
      ],
      "metadata": {
        "id": "nx2kYJF-7s_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function to shuffle features and labels list Together"
      ],
      "metadata": {
        "id": "Ld_tFDxMT2W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_together(list1, list2):\n",
        "    \"\"\"\n",
        "    Shuffle two lists in unison.\n",
        "\n",
        "    Parameters:\n",
        "    - list1: The first list to shuffle.\n",
        "    - list2: The second list to shuffle, must be the same length as list1.\n",
        "\n",
        "    Returns:\n",
        "    - The shuffled list1 and list2.\n",
        "    \"\"\"\n",
        "    if len(list1) != len(list2):\n",
        "        raise ValueError(\"The lists to be shuffled must be the same length.\")\n",
        "\n",
        "    # sklearn's shuffle function to shuffle both lists in unison\n",
        "    list1_shuffled, list2_shuffled = shuffle(list1, list2)\n",
        "    return list1_shuffled, list2_shuffled\n"
      ],
      "metadata": {
        "id": "-MnJMvvu7uZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Resize, normalize and split videos into frames. Batch frames in 32 segments.\n",
        "\n",
        "Use data generators to prevent loading large datset into memory at once, to better utilize memory for resource constrained environement.\n",
        "\n",
        "Generator loads and preprocess 32 frames at each interval."
      ],
      "metadata": {
        "id": "3Pm7z6-3UOec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_segment_video(video_path, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Preprocess raw videos by resizing, color space conversion, normalization\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: absolute path to video file\n",
        "    - resize_shape: shape (224,224) to resize video to\n",
        "    - segment_length: value to segment frames of video to\n",
        "\n",
        "    Returns:\n",
        "    - a batch of 32 preprocessed frames of shape (32, 224, 224, 3), using generators\n",
        "    \"\"\"\n",
        "    def video_generator():\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break  # Exit loop if video ends\n",
        "            frame = cv2.resize(frame, resize_shape)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255.0\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == segment_length:\n",
        "                yield np.array(frames)\n",
        "                frames = []\n",
        "\n",
        "        # Handle the last segment if it has fewer frames than segment_length\n",
        "        if frames:  # Check if there are remaining frames\n",
        "            # Pad the last segment to have 'segment_length' frames\n",
        "            padding = [np.zeros_like(frames[0]) for _ in range(segment_length - len(frames))]\n",
        "            frames.extend(padding)\n",
        "            yield np.array(frames)  # Yield the padded last segment\n",
        "        cap.release()\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        video_generator,\n",
        "        output_signature=tf.TensorSpec(shape=(segment_length, *resize_shape, 3), dtype=tf.float32)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "77GImlkFzaPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Dataset into Training & Validation Sets"
      ],
      "metadata": {
        "id": "BIjfSbVTWdsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_from_videos(video_paths, labels, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Creates a TensorFlow dataset of video segments with corresponding labels.\n",
        "\n",
        "    Args:\n",
        "        video_paths (list of str): Paths to video files.\n",
        "        labels (list of int): Labels for each video file.\n",
        "        resize_shape (tuple): The target shape for resizing frames.\n",
        "        segment_length (int): Number of frames per video segment.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A dataset of video segments and labels.\n",
        "    \"\"\"\n",
        "    def generator():\n",
        "        for video_path, label in zip(video_paths, labels):\n",
        "            # Use preprocess_and_segment_video to yield segments\n",
        "            for segment in preprocess_and_segment_video(video_path, resize_shape, segment_length):\n",
        "                yield segment.numpy(), label\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(segment_length, *resize_shape, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_train_val_datasets(anomaly_dir, normal_dir, train_size=0.8, shuffle_buffer_size=100):\n",
        "    \"\"\"\n",
        "    Create training and validation set\n",
        "\n",
        "    Args:\n",
        "        anomaly_dir: Path to anomalous videos\n",
        "        normal_dir: Path to normal videos\n",
        "        train_size: splitting size for training\n",
        "        shuffle_buffer_size: shuffle size\n",
        "\n",
        "    Returns:\n",
        "        train_dataset, val_dataset\n",
        "    \"\"\"\n",
        "    # list all video files in a directory and their labels\n",
        "    anomaly_videos, normal_videos = list_video_files(anomaly_dir), list_video_files(normal_dir)\n",
        "    video_paths = anomaly_videos + normal_videos\n",
        "    labels = [1] * len(anomaly_videos) + [0] * len(normal_videos)\n",
        "\n",
        "    # Shuffle video_paths and labels in unison\n",
        "    video_paths, labels = shuffle_together(video_paths, labels)\n",
        "\n",
        "    # Create a combined dataset\n",
        "    dataset = create_dataset_from_videos(video_paths, labels)\n",
        "\n",
        "    # Calculate dataset sizes\n",
        "    total_size = len(video_paths)\n",
        "    train_size = int(total_size * train_size)\n",
        "\n",
        "    # Split the dataset\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    val_dataset = dataset.skip(train_size)\n",
        "\n",
        "    return train_dataset, val_dataset\n"
      ],
      "metadata": {
        "id": "4p6erhh4zIII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tune A Pretrained Video Swin Transformer Model for Video Classification"
      ],
      "metadata": {
        "id": "v4B-y1mRl1Pm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzq9StfOfWfv"
      },
      "outputs": [],
      "source": [
        "def build_finetune_model():\n",
        "    \"\"\"\n",
        "    Fine tune pretrained model\n",
        "\n",
        "    Returns:\n",
        "        model (Fine-tuned for shooting classification)\n",
        "    \"\"\"\n",
        "   # import pretrained model, i.e.\n",
        "    video_swin = keras.models.load_model(saved_model_path, compile= False)\n",
        "    video_swin.trainable = False\n",
        "\n",
        "    # downstream model\n",
        "    model = keras.Sequential([\n",
        "        video_swin,\n",
        "        # video_swin outputs a 1D tensor of shape (600,)\n",
        "        layers.Dense(1024, activation='relu'),  # Additional Dense layer\n",
        "        layers.Dropout(0.5),  # Dropout layer to reduce overfitting\n",
        "        layers.Dense(1, activation='sigmoid')  # Final Dense layer for binary classification\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Fine-Tuned Model for Shooting Classification"
      ],
      "metadata": {
        "id": "z7RHFSSomcmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and validation data\n",
        "train_dataset, val_dataset = create_train_val_datasets(anomaly_dir, normal_dir, train_size=0.8, shuffle_buffer_size=100)\n",
        "\n",
        "# Batch for memory efficiency\n",
        "batch_size = 1\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# Asynchronously fetch batches while model is training\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Only use cache() if memory limits won't be exceeded\n",
        "#train_dataset = train_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "#val_dataset = val_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "finetune_model = build_finetune_model()\n",
        "# Compile model with binary entropy loss, adam optimizer and metrics\n",
        "finetune_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = finetune_model.fit(train_dataset, epochs=10,validation_data=val_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "iUKmgvFm43KK",
        "outputId": "c71f424d-ed6a-4b6d-e27f-02d86fcff428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "     41/Unknown - 880s 20s/step - loss: 0.0180 - accuracy: 0.9756 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-386a37ff1ef9>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetune_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Final Model Performance"
      ],
      "metadata": {
        "id": "ced1WK7dnwdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot precision, recall, and AUC\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Precision\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['precision'], label='Train Precision')\n",
        "plt.plot(history.history['val_precision'], label='Validation Precision')\n",
        "plt.title('Precision over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "\n",
        "# Recall\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['recall'], label='Train Recall')\n",
        "plt.plot(history.history['val_recall'], label='Validation Recall')\n",
        "plt.title('Recall over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['auc'], label='Train AUC')\n",
        "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "plt.title('AUC over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "#test_loss, test_accuracy, test_precision, test_recall, test_auc = finetune_model.evaluate(test_dataset)\n",
        "#print(f\"Test Metrics:\\n Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, AUC: {test_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "7VI6xmkh497Z",
        "outputId": "08c38b1c-5d45-4014-fd2c-9373f2eab700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cce34d2bd6d6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy over epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References:\n",
        "\n",
        "*   Sultani, W., Chen, C. and Shah, M., 2018. Real-world anomaly detection in surveillance videos. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Available at: https://www.crcv.ucf.edu/research/real-world-anomaly-detection-in-surveillance-videos/ 20/03/2024.\n",
        "*   Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L. and Wei, F., 2022. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Available at: https://github.com/innat/VideoSwin 20/03/2024\n"
      ],
      "metadata": {
        "id": "Aq1hW67sMce9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ydJFOP4CMdhA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gynqv509-foIt7uHod2zNMODzUkFN7MN",
      "authorship_tag": "ABX9TyOmB4G6Ua5LkZliy9+2uXfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}