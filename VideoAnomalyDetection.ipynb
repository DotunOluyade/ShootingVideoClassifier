{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DotunOluyade/ShootingVideoClassifier/blob/main/VideoAnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqG2VmTbSxG7"
      },
      "source": [
        "\n",
        "# SwinAnomaly Transformer-Model for Shooting Anomaly Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run this Cell First for Custome File Import"
      ],
      "metadata": {
        "id": "A8jrVYC_25lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/VideoAnomalyDetection/*.py /content/\n",
        "\n",
        "\"\"\"\n",
        "TensorFlow version: 2.15.0\n",
        "Keras version: 2.15.0\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tbmNDWy78_6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0abe91d-4667-408c-d016-5679d7c346a9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTensorFlow version: 2.15.0\\nKeras version: 2.15.0\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuned Pretrained VideoSwin Model"
      ],
      "metadata": {
        "id": "JjUkliUBIxK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import TimeDistributed, Dense, Flatten, GlobalMaxPooling1D\n",
        "\n",
        "def build_finetuned_model(pretrained_model_path, segment_count=32):\n",
        "    \"\"\"\n",
        "    Fine-tune a pretrained model for shooting classification.\n",
        "\n",
        "    Args:\n",
        "        saved_model_path (str): Path to the saved Video Swin Transformer model.\n",
        "\n",
        "    Returns:\n",
        "        model (keras.Model): Fine-tuned model.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Loading the pretrained model from:\", pretrained_model_path)\n",
        "    pretrained_model = load_model(pretrained_model_path, compile=False)\n",
        "\n",
        "    print(\"Freeze all layers to prevent updates to weights\")\n",
        "    # Prevent the pre-trained weights from being updated during training\n",
        "    pretrained_model.trainable = False\n",
        "\n",
        "    print(\"Display pretrained base model architecture:\")\n",
        "    pretrained_model.summary()\n",
        "\n",
        "    # Fine tune the architecture by connecting the pretrainined model to a downstream architecture for\n",
        "    # shooting anomaly detection\n",
        "    print(\"Build Fine-tuned model\")\n",
        "    model = Sequential([\n",
        "        TimeDistributed(pretrained_model, input_shape=(segment_count,) + pretrained_model.input_shape[1:]),\n",
        "        TimeDistributed(Dense(1, activation='sigmoid')),  # Use sigmoid for binary output\n",
        "        GlobalMaxPooling1D()  # Use max pooling to select the highest score as the output\n",
        "    ], name='anomaly_detection_model')\n",
        "\n",
        "    print(\"Display fine-tuned model architecture:\")\n",
        "    model.summary()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "tUmnF3RLEl_5"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "iUKmgvFm43KK",
        "outputId": "73715661-c697-4657-eb9b-3bd8b0383588"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-96a1bd20fb9d>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mepoch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_train_val_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresize_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Calculate steps per epoch for training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC, Accuracy, F1Score\n",
        "import pickle\n",
        "import os\n",
        "import config\n",
        "import numpy as np\n",
        "#import model\n",
        "import video_utils as vu\n",
        "import callbacks\n",
        "import time\n",
        "import data_processor as dp\n",
        "import mil_loss as mil\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "train_file_path = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Anomaly_Train_2.txt'\n",
        "segments_dir = '/content/drive/MyDrive/VideoAnomalyDetection/output/preprocessed_videos'\n",
        "base_dataset_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/'\n",
        "base_directory = '/content/drive/MyDrive/VideoAnomalyDetection/'\n",
        "pretrained_model_path = '/content/drive/MyDrive/VideoAnomalyDetection/pretrained/Video Swin Transformer/TFVideoSwinB_K600_IN22K_P244_W877_32x224'\n",
        "\n",
        "model_chkpt_filename = f\"training_1/vad_{int(time.time())}.ckpt\"\n",
        "checkpoint_path = os.path.join(base_directory, model_chkpt_filename)\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Read video paths from the file\n",
        "video_paths = vu.read_video_paths_from_file(train_file_path,base_dataset_dir)\n",
        "\n",
        "# Use \"Shooting\" and \"Normal\" to distinguish class types\n",
        "anomaly_videos = vu.filter_video_paths(video_paths, 'Shooting')\n",
        "normal_videos = vu.filter_video_paths(video_paths, 'Normal')\n",
        "\n",
        "anomaly_videos= vu.select_random_videos(anomaly_videos, 4)\n",
        "normal_videos= vu.select_random_videos(normal_videos, 4)\n",
        "\n",
        "video_paths = anomaly_videos + normal_videos\n",
        "\n",
        "# Annotate anomalous and normal videos\n",
        "labels = [1] * len(anomaly_videos) + [0] * len(normal_videos)\n",
        "\n",
        "# Alternate normal and anomalous videos to ensure pairs are passed along for training\n",
        "video_paths, labels = vu.alternate_video_and_labels(video_paths, labels)\n",
        "\n",
        "batch_size=32\n",
        "epoch_size = 10\n",
        "\n",
        "train_dataset, val_dataset, total_train_samples, total_val_samples = dp.create_train_val_datasets(video_paths, labels,resize_shape=(224, 224), segment_length=32, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Calculate steps per epoch for training and validation\n",
        "steps_per_epoch = total_train_samples // batch_size\n",
        "validation_steps = total_val_samples // batch_size\n",
        "\n",
        "# Ensure datasets are shuffled\n",
        "train_dataset = train_dataset.shuffle(buffer_size=total_train_samples,reshuffle_each_iteration=True)\n",
        "val_dataset = val_dataset.shuffle(buffer_size=total_val_samples,reshuffle_each_iteration=True)\n",
        "\n",
        "# Fetch batches while model is training\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Configure the Adam optimizer with a conventional learning rate\n",
        "adam_optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Load the pretrained Video Swin model\n",
        "finetuned_model = build_finetuned_model(pretrained_model_path, batch_size)\n",
        "\n",
        "# If there exists a previous checkpoint, load it\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "if latest:\n",
        "    print(f\"Resuming training from {latest}\")\n",
        "    finetuned_model.load_weights(latest)\n",
        "\n",
        "# Compile model with MIL loss, adam optimizer and metrics\n",
        "finetuned_model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=mil.mil_ranking_loss,\n",
        "    #loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        Accuracy(name='accuracy'),\n",
        "        Precision(name='precision'),\n",
        "        Recall(name='recall'),\n",
        "        AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = finetuned_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epoch_size,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=callbacks.callbacks_list,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save history object\n",
        "with open('model_history.pkl', 'wb') as file:\n",
        "    pickle.dump(history.history, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7QgYgQ9azdi",
        "outputId": "bcae724d-e1a3-4ec5-8fbd-1f7afcb144f5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Fine-tuned Model"
      ],
      "metadata": {
        "id": "XIUG_fTzJ988"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDvCgaD_KBNQ",
        "outputId": "acfd8d0b-e43e-448f-99c3-3f88a5977f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1369/1369 [==============================] - ETA: 0s - loss: 0.1809 - accuracy: 0.6676 - precision: 0.9910 - recall: 0.9940 - auc: 0.9457\n",
            "Epoch 1: val_loss improved from inf to 0.12093, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1713886874.ckpt\n",
            "1369/1369 [==============================] - 1072s 700ms/step - loss: 0.1809 - accuracy: 0.6676 - precision: 0.9910 - recall: 0.9940 - auc: 0.9457 - val_loss: 0.1209 - val_accuracy: 0.9767 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1369/1369 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.8503 - precision: 0.9978 - recall: 0.9993 - auc: 0.9844\n",
            "Epoch 2: val_loss improved from 0.12093 to 0.12002, saving model to /content/drive/MyDrive/VideoAnomalyDetection/training_1/vad_1713886874.ckpt\n",
            "1369/1369 [==============================] - 993s 725ms/step - loss: 0.1320 - accuracy: 0.8503 - precision: 0.9978 - recall: 0.9993 - auc: 0.9844 - val_loss: 0.1200 - val_accuracy: 0.9767 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1171/1369 [========================>.....] - ETA: 1:44 - loss: 0.1271 - accuracy: 0.8676 - precision: 0.9965 - recall: 0.9991 - auc: 0.9996"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ced1WK7dnwdb"
      },
      "source": [
        "# Post Training Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VI6xmkh497Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e3b4284a-0840-4868-bf0c-b5963289fc22"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'model_history.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0f1cdfba08b2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the history from a pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_history.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloaded_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_history.pkl'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load the history from a pickle file\n",
        "with open('model_history.pkl', 'rb') as file:\n",
        "    loaded_history = pickle.load(file)\n",
        "\n",
        "print(loaded_history)\n",
        "\n",
        "\n",
        "history_df = pd.DataFrame(loaded_history)\n",
        "\n",
        "# Number of metrics to plot\n",
        "num_metrics = len(history_df.columns)\n",
        "# Define how many columns of subplots\n",
        "num_columns = 2\n",
        "num_rows = (num_metrics + 1) // num_columns\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "\n",
        "for i, col in enumerate(history_df.columns):\n",
        "    ax = axes.flatten()[i]\n",
        "    history_df[[col]].plot(ax=ax)\n",
        "    ax.set_title(col)\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel(col)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Display the metrics in a tabular format using DataFrame\n",
        "print(history_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "# Make prediction with validation dataset that includes labels\n",
        "val_predictions = finetune_model.predict(val_dataset)\n",
        "# Get the index of the max prediction\n",
        "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "true_labels = np.concatenate([y.numpy() for _, y in val_dataset])\n",
        "\n",
        "cm = confusion_matrix(true_labels, val_pred_labels)\n",
        "\n",
        "# Using matplotlib\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(set(true_labels)))  # Number of classes\n",
        "plt.xticks(tick_marks, range(len(set(true_labels))), rotation=45)\n",
        "plt.yticks(tick_marks, range(len(set(true_labels))))\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, cm[i, j],\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n",
        "\n",
        "# Using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5, cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OmiIQH9ChB9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}